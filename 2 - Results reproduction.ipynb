{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 08:18:20.210697: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 08:18:20.211572: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 08:18:20.216088: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 08:18:20.227760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746260300.248224  182071 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746260300.254205  182071 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746260300.269535  182071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746260300.269555  182071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746260300.269557  182071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746260300.269558  182071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 08:18:20.276562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "from src import baseline_model, frozen_model, adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55493ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('https://minio.lab.sspcloud.fr/nguibe/NLP/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192611c4",
   "metadata": {},
   "source": [
    "# 1 - First result reproduction: Performance drop from English-only fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004cdf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5\n",
    "test_size = 5\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748acb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 302.79 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.09 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 142.16 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 136.83 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.53 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filev8y57kk3.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1436, in call  *\n            outputs = self.roberta(\n        File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFXLMRobertaMainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 822, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer 'roberta' (type TFXLMRobertaMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 512), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, None, 512), dtype=int64)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mbaseline_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_sample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mtest_sample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/NLP-Legal-document-classification/src/baseline_model.py:81\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m(data, train_sample_size, test_sample_size, batch_size, epochs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Train and evaluate with reusable functions\u001b[39;00m\n\u001b[32m     80\u001b[39m train_tf_batched = train_tf.batch(batch_size)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m training_time, initial_memory, final_memory = \u001b[43mtrack_training_time_and_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tf_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Evaluate on all test languages using existing evaluate_model()\u001b[39;00m\n\u001b[32m     86\u001b[39m results = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/NLP-Legal-document-classification/src/utils.py:109\u001b[39m, in \u001b[36mtrack_training_time_and_memory\u001b[39m\u001b[34m(model, train_dataset, batch_size, epochs)\u001b[39m\n\u001b[32m    106\u001b[39m initial_memory = process.memory_info().rss / \u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# in MB\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Get final memory usage\u001b[39;00m\n\u001b[32m    112\u001b[39m final_memory = process.memory_info().rss / \u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# in MB\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1209\u001b[39m, in \u001b[36mTFPreTrainedModel.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(keras.Model.fit)\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1208\u001b[39m     args, kwargs = convert_batch_encoding(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_file40bmk_um.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1652\u001b[39m, in \u001b[36mTFPreTrainedModel.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1650\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(x, training=\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._using_dummy_loss:\n\u001b[32m   1654\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compiled_loss(y_pred.loss, y_pred.loss, sample_weight, regularization_losses=\u001b[38;5;28mself\u001b[39m.losses)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileztupeatj.py:37\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(**ag__.ld(unpacked_inputs)), fscope)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filev8y57kk3.py:17\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[39m\n\u001b[32m     15\u001b[39m do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     16\u001b[39m retval_ = ag__.UndefinedReturnValue()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m outputs = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m sequence_output = ag__.ld(outputs)[\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m logits = ag__.converted_call(ag__.ld(\u001b[38;5;28mself\u001b[39m).classifier, (ag__.ld(sequence_output),), \u001b[38;5;28mdict\u001b[39m(training=ag__.ld(training)), fscope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileztupeatj.py:37\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(**ag__.ld(unpacked_inputs)), fscope)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filee_elriq6.py:76\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[39m\n\u001b[32m     74\u001b[39m input_shape = ag__.Undefined(\u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m ag__.if_stmt(ag__.and_(\u001b[38;5;28;01mlambda\u001b[39;00m: ag__.ld(input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: ag__.ld(inputs_embeds) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m,), \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m batch_size, seq_length = ag__.ld(input_shape)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_4\u001b[39m():\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filev8y57kk3.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1436, in call  *\n            outputs = self.roberta(\n        File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFXLMRobertaMainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 822, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer 'roberta' (type TFXLMRobertaMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 512), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, None, 512), dtype=int64)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation\n",
    "results = baseline_model.run_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c655d7d",
   "metadata": {},
   "source": [
    "## Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dc635",
   "metadata": {},
   "source": [
    "Investigating catastrophic forgetting more deeply by comparing the weight shifts between a pre-trained multilingual model and a retrained, English-only model could provide a more granular understanding of how language-specific information is lost during retraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_difference(pretrained_weights, retrained_weights):\n",
    "    differences = []\n",
    "    for pretrained, retrained in zip(pretrained_weights, retrained_weights):\n",
    "        diff = np.linalg.norm(pretrained - retrained)  # L2 norm for weight difference\n",
    "        differences.append(diff)\n",
    "    return differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a15b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve weights\n",
    "pretrained_weights = model.get_weights()  # Pretrained model weights\n",
    "retrained_weights = model.get_weights()   # Retrained model weights\n",
    "\n",
    "# Compute the differences between pre-trained and retrained model\n",
    "weight_differences = compute_weight_difference(pretrained_weights, retrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize or identify which layers show the most significant changes\n",
    "most_changed_layers = np.argsort(weight_differences)[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce577d2",
   "metadata": {},
   "source": [
    "3.\tVisualize the Results:\n",
    "\n",
    "o\tYou could create a heatmap or bar chart to visualize the magnitude of the changes in each layer.\n",
    "\n",
    "o\tLayer-wise comparison (e.g., attention heads, feed-forward layers, etc.) could reveal which part of the model \"forgot\" multilingual information the most.\n",
    "\n",
    "4.\tInterpretation of Results:\n",
    "\n",
    "o\tBy examining the most significantly changed layers, you could hypothesize which parts of the model are most sensitive to language-specific data. For example:\n",
    "\n",
    "\tAttention layers might \"forget\" multilingual information, especially if they were tuned heavily toward English sentence structure.\n",
    "\n",
    "\tEmbedding layers might change more dramatically as they encode language-specific token representations.\n",
    "\n",
    "o\tComparing only certain parts of the network, like the early transformer layers (closer to embeddings), versus the deeper layers (closer to output), could give insights into how the model's ability to generalize across languages is disrupted.\n",
    "\n",
    "________________________________________\n",
    "Challenges:\n",
    "\n",
    "•\tScale: Since models like XLM-Roberta are large (hundreds of millions of parameters), computing and storing weight differences across all layers could be resource-intensive.\n",
    "\n",
    "•\tLayer-specific analysis: Some layers (e.g., final layers or task-specific heads) may change more due to the retraining task (e.g., classification), so you’d want to focus on layers like attention and embeddings that are more involved in the multilingual representation.\n",
    "\n",
    "•\tLearning rate and retraining settings: Differences in fine-tuning settings (like learning rate) could also cause changes in specific parts of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade86b66",
   "metadata": {},
   "source": [
    "# 2 - Second result reproduction: \"better\" performance with adaptation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a99999",
   "metadata": {},
   "source": [
    "## Frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c58724",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1110.59 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:01<00:00, 569.17 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:01<00:00, 894.23 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:01<00:00, 518.60 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:01<00:00, 552.57 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:01<00:00, 600.68 examples/s]\n",
      "2025-05-03 08:19:35.269605: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully froze first 6 transformer layers.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 08:20:13.332894: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 1957s 14s/step - loss: 0.3556 - auc: 0.4966"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = frozen_model.run_training_pipeline_with_freezing(df=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs, n_frozen_layer= N)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc55bd8",
   "metadata": {},
   "source": [
    "## Adaptaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb9d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:14<00:00, 335.83 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:05<00:00, 173.12 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:04<00:00, 212.96 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:06<00:00, 166.37 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:06<00:00, 146.61 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:06<00:00, 150.48 examples/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m980s\u001b[0m 6s/step - auc_3: 0.5002 - loss: 0.4051\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1506s\u001b[0m 10s/step - auc_3: 0.5266 - loss: 0.3229\n",
      "Training time: 2486.09 seconds\n",
      "Initial memory usage: 16112.71 MB\n",
      "Final memory usage: 15944.74 MB\n",
      "Memory used during training: -167.97 MB\n",
      "[INFO] Evaluating on language: de\n",
      "R-Precision: 0.2873\n",
      "Micro F1: 0.4511\n",
      "Macro F1: 0.1413\n",
      "LRAP: 0.6569\n",
      "Evaluation time: 321.10 seconds\n",
      "[INFO] Evaluating on language: en\n",
      "R-Precision: 0.2905\n",
      "Micro F1: 0.4477\n",
      "Macro F1: 0.1525\n",
      "LRAP: 0.6519\n",
      "Evaluation time: 193.24 seconds\n",
      "[INFO] Evaluating on language: fi\n",
      "R-Precision: 0.2922\n",
      "Micro F1: 0.4409\n",
      "Macro F1: 0.1389\n",
      "LRAP: 0.6331\n",
      "Evaluation time: 176.75 seconds\n",
      "[INFO] Evaluating on language: fr\n",
      "R-Precision: 0.2888\n",
      "Micro F1: 0.4470\n",
      "Macro F1: 0.1265\n",
      "LRAP: 0.6357\n",
      "Evaluation time: 180.75 seconds\n",
      "[INFO] Evaluating on language: pl\n",
      "R-Precision: 0.2884\n",
      "Micro F1: 0.4238\n",
      "Macro F1: 0.1401\n",
      "LRAP: 0.6204\n",
      "Evaluation time: 239.95 seconds\n",
      "{'de': {'Eval Time (s)': 321.10262393951416,\n",
      "        'LRAP': 0.6569023707135659,\n",
      "        'Macro F1': 0.1413412282571887,\n",
      "        'Micro F1': 0.45113077679449365,\n",
      "        'R-Precision': 0.2873353596757852},\n",
      " 'en': {'Eval Time (s)': 193.23800039291382,\n",
      "        'LRAP': 0.651868276506948,\n",
      "        'Macro F1': 0.15254882189618896,\n",
      "        'Micro F1': 0.4476711295493117,\n",
      "        'R-Precision': 0.29052734375},\n",
      " 'fi': {'Eval Time (s)': 176.74569249153137,\n",
      "        'LRAP': 0.6331163942550738,\n",
      "        'Macro F1': 0.1388955059600318,\n",
      "        'Micro F1': 0.44087075897234757,\n",
      "        'R-Precision': 0.29222333000997014},\n",
      " 'fr': {'Eval Time (s)': 180.7537190914154,\n",
      "        'LRAP': 0.6356858038005143,\n",
      "        'Macro F1': 0.12653720033701407,\n",
      "        'Micro F1': 0.4470235866716586,\n",
      "        'R-Precision': 0.28875739644970416},\n",
      " 'pl': {'Eval Time (s)': 239.95244479179382,\n",
      "        'LRAP': 0.620393191319249,\n",
      "        'Macro F1': 0.1401362300381464,\n",
      "        'Micro F1': 0.423766364551863,\n",
      "        'R-Precision': 0.2883744855967078}}\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = adapter_model.run_adapter_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
