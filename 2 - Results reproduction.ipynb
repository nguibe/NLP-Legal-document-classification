{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 17:01:13.328979: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 17:01:13.330555: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 17:01:13.337237: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 17:01:13.349328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746291673.369711  223430 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746291673.375702  223430 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746291673.390896  223430 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746291673.390914  223430 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746291673.390916  223430 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746291673.390917  223430 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 17:01:13.396536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "from src import baseline_model, frozen_model, adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55493ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('https://minio.lab.sspcloud.fr/nguibe/NLP/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192611c4",
   "metadata": {},
   "source": [
    "# 1 - First result reproduction: Performance drop from English-only fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748acb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1102.49 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:02<00:00, 472.89 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:01<00:00, 760.46 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:01<00:00, 566.83 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:02<00:00, 489.41 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:01<00:00, 556.74 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "     54/Unknown - 826s 15s/step - loss: 0.3683 - auc_3: 0.4961"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation\n",
    "results = baseline_model.run_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c655d7d",
   "metadata": {},
   "source": [
    "## Performance analysis - draft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dc635",
   "metadata": {},
   "source": [
    "This section was intended to explore weight shifts between a pre-trained multilingual model and an English-only retrained version, as a way to quantify potential catastrophic forgetting.\n",
    "\n",
    "Due to time constraints, this analysis was not completed. However, the following code sketch could be used to pursue this direction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft code for future weight comparison analysis\n",
    "\n",
    "def compute_weight_difference(pretrained_weights, retrained_weights):\n",
    "    differences = []\n",
    "    for pretrained, retrained in zip(pretrained_weights, retrained_weights):\n",
    "        diff = np.linalg.norm(pretrained - retrained)  # L2 norm\n",
    "        differences.append(diff)\n",
    "    return differences\n",
    "\n",
    "# Example usage:\n",
    "# pretrained_weights = model_pretrained.get_weights()\n",
    "# retrained_weights = model_retrained.get_weights()\n",
    "# weight_differences = compute_weight_difference(pretrained_weights, retrained_weights)\n",
    "# most_changed_layers = np.argsort(weight_differences)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade86b66",
   "metadata": {},
   "source": [
    "# 2 - Second result reproduction: \"better\" performance with adaptation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a99999",
   "metadata": {},
   "source": [
    "## Frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c58724",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6667a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1191.71 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:01<00:00, 499.02 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:01<00:00, 950.96 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:01<00:00, 588.22 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:01<00:00, 543.28 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:01<00:00, 569.87 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully froze first 6 transformer layers.\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 2113s 13s/step - loss: 0.3538 - auc_1: 0.4988\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 2108s 13s/step - loss: 0.3468 - auc_1: 0.4992\n",
      "Training time: 4220.72 seconds\n",
      "Initial memory usage: 13012.20 MB\n",
      "Final memory usage: 40761.23 MB\n",
      "Memory used during training: 27749.03 MB\n",
      "\n",
      "[INFO] Evaluating for de\n",
      "R-Precision: 0.2700\n",
      "Micro F1: 0.2289\n",
      "Macro F1: 0.0329\n",
      "LRAP: 0.5416\n",
      "Evaluation time: 156.76 seconds\n",
      "\n",
      "[INFO] Evaluating for en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:20:21.151905: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Precision: 0.2688\n",
      "Micro F1: 0.2366\n",
      "Macro F1: 0.0335\n",
      "LRAP: 0.5417\n",
      "Evaluation time: 164.81 seconds\n",
      "\n",
      "[INFO] Evaluating for fi\n",
      "R-Precision: 0.2687\n",
      "Micro F1: 0.2309\n",
      "Macro F1: 0.0331\n",
      "LRAP: 0.5330\n",
      "Evaluation time: 159.60 seconds\n",
      "\n",
      "[INFO] Evaluating for fr\n",
      "R-Precision: 0.2740\n",
      "Micro F1: 0.2419\n",
      "Macro F1: 0.0339\n",
      "LRAP: 0.5527\n",
      "Evaluation time: 160.89 seconds\n",
      "\n",
      "[INFO] Evaluating for pl\n",
      "R-Precision: 0.2711\n",
      "Micro F1: 0.2399\n",
      "Macro F1: 0.0342\n",
      "LRAP: 0.5387\n",
      "Evaluation time: 154.70 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = frozen_model.run_training_pipeline_with_freezing(df=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs, n_frozen_layer= N)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc55bd8",
   "metadata": {},
   "source": [
    "## Adaptaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb9d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:14<00:00, 335.83 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:05<00:00, 173.12 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:04<00:00, 212.96 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:06<00:00, 166.37 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:06<00:00, 146.61 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:06<00:00, 150.48 examples/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m980s\u001b[0m 6s/step - auc_3: 0.5002 - loss: 0.4051\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1506s\u001b[0m 10s/step - auc_3: 0.5266 - loss: 0.3229\n",
      "Training time: 2486.09 seconds\n",
      "Initial memory usage: 16112.71 MB\n",
      "Final memory usage: 15944.74 MB\n",
      "Memory used during training: -167.97 MB\n",
      "[INFO] Evaluating on language: de\n",
      "R-Precision: 0.2873\n",
      "Micro F1: 0.4511\n",
      "Macro F1: 0.1413\n",
      "LRAP: 0.6569\n",
      "Evaluation time: 321.10 seconds\n",
      "[INFO] Evaluating on language: en\n",
      "R-Precision: 0.2905\n",
      "Micro F1: 0.4477\n",
      "Macro F1: 0.1525\n",
      "LRAP: 0.6519\n",
      "Evaluation time: 193.24 seconds\n",
      "[INFO] Evaluating on language: fi\n",
      "R-Precision: 0.2922\n",
      "Micro F1: 0.4409\n",
      "Macro F1: 0.1389\n",
      "LRAP: 0.6331\n",
      "Evaluation time: 176.75 seconds\n",
      "[INFO] Evaluating on language: fr\n",
      "R-Precision: 0.2888\n",
      "Micro F1: 0.4470\n",
      "Macro F1: 0.1265\n",
      "LRAP: 0.6357\n",
      "Evaluation time: 180.75 seconds\n",
      "[INFO] Evaluating on language: pl\n",
      "R-Precision: 0.2884\n",
      "Micro F1: 0.4238\n",
      "Macro F1: 0.1401\n",
      "LRAP: 0.6204\n",
      "Evaluation time: 239.95 seconds\n",
      "{'de': {'Eval Time (s)': 321.10262393951416,\n",
      "        'LRAP': 0.6569023707135659,\n",
      "        'Macro F1': 0.1413412282571887,\n",
      "        'Micro F1': 0.45113077679449365,\n",
      "        'R-Precision': 0.2873353596757852},\n",
      " 'en': {'Eval Time (s)': 193.23800039291382,\n",
      "        'LRAP': 0.651868276506948,\n",
      "        'Macro F1': 0.15254882189618896,\n",
      "        'Micro F1': 0.4476711295493117,\n",
      "        'R-Precision': 0.29052734375},\n",
      " 'fi': {'Eval Time (s)': 176.74569249153137,\n",
      "        'LRAP': 0.6331163942550738,\n",
      "        'Macro F1': 0.1388955059600318,\n",
      "        'Micro F1': 0.44087075897234757,\n",
      "        'R-Precision': 0.29222333000997014},\n",
      " 'fr': {'Eval Time (s)': 180.7537190914154,\n",
      "        'LRAP': 0.6356858038005143,\n",
      "        'Macro F1': 0.12653720033701407,\n",
      "        'Micro F1': 0.4470235866716586,\n",
      "        'R-Precision': 0.28875739644970416},\n",
      " 'pl': {'Eval Time (s)': 239.95244479179382,\n",
      "        'LRAP': 0.620393191319249,\n",
      "        'Macro F1': 0.1401362300381464,\n",
      "        'Micro F1': 0.423766364551863,\n",
      "        'R-Precision': 0.2883744855967078}}\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = adapter_model.run_adapter_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Results will appear as log but can also be displayed with:\n",
    "#import pprint\n",
    "#pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
