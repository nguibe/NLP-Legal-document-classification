{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 03:53:43.964908: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 03:53:43.965764: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 03:53:43.970454: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 03:53:43.982717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746244424.003592  151792 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746244424.010002  151792 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746244424.025992  151792 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746244424.026013  151792 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746244424.026015  151792 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746244424.026016  151792 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 03:53:44.031765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "from src import baseline_model, frozen_model, adapter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55493ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('https://minio.lab.sspcloud.fr/nguibe/NLP/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192611c4",
   "metadata": {},
   "source": [
    "# 1 - First result reproduction: Performance drop from English-only fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004cdf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5\n",
    "test_size = 5\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748acb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 302.79 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.09 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 142.16 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 136.83 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.53 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filev8y57kk3.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1436, in call  *\n            outputs = self.roberta(\n        File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFXLMRobertaMainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 822, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer 'roberta' (type TFXLMRobertaMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 512), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, None, 512), dtype=int64)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mbaseline_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_sample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mtest_sample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/NLP-Legal-document-classification/src/baseline_model.py:81\u001b[39m, in \u001b[36mrun_training_pipeline\u001b[39m\u001b[34m(data, train_sample_size, test_sample_size, batch_size, epochs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Train and evaluate with reusable functions\u001b[39;00m\n\u001b[32m     80\u001b[39m train_tf_batched = train_tf.batch(batch_size)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m training_time, initial_memory, final_memory = \u001b[43mtrack_training_time_and_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tf_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Evaluate on all test languages using existing evaluate_model()\u001b[39;00m\n\u001b[32m     86\u001b[39m results = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/NLP-Legal-document-classification/src/utils.py:109\u001b[39m, in \u001b[36mtrack_training_time_and_memory\u001b[39m\u001b[34m(model, train_dataset, batch_size, epochs)\u001b[39m\n\u001b[32m    106\u001b[39m initial_memory = process.memory_info().rss / \u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# in MB\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Get final memory usage\u001b[39;00m\n\u001b[32m    112\u001b[39m final_memory = process.memory_info().rss / \u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# in MB\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1209\u001b[39m, in \u001b[36mTFPreTrainedModel.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(keras.Model.fit)\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1208\u001b[39m     args, kwargs = convert_batch_encoding(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_file40bmk_um.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1652\u001b[39m, in \u001b[36mTFPreTrainedModel.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1650\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(x, training=\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._using_dummy_loss:\n\u001b[32m   1654\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compiled_loss(y_pred.loss, y_pred.loss, sample_weight, regularization_losses=\u001b[38;5;28mself\u001b[39m.losses)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileztupeatj.py:37\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(**ag__.ld(unpacked_inputs)), fscope)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filev8y57kk3.py:17\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[39m\n\u001b[32m     15\u001b[39m do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     16\u001b[39m retval_ = ag__.UndefinedReturnValue()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m outputs = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m sequence_output = ag__.ld(outputs)[\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m logits = ag__.converted_call(ag__.ld(\u001b[38;5;28mself\u001b[39m).classifier, (ag__.ld(sequence_output),), \u001b[38;5;28mdict\u001b[39m(training=ag__.ld(training)), fscope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileztupeatj.py:37\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(**ag__.ld(unpacked_inputs)), fscope)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filee_elriq6.py:76\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[39m\n\u001b[32m     74\u001b[39m input_shape = ag__.Undefined(\u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m ag__.if_stmt(ag__.and_(\u001b[38;5;28;01mlambda\u001b[39;00m: ag__.ld(input_ids) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: ag__.ld(inputs_embeds) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[33m'\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m'\u001b[39m,), \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m batch_size, seq_length = ag__.ld(input_shape)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_4\u001b[39m():\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filev8y57kk3.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).roberta, (ag__.ld(input_ids),), dict(attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n        batch_size, seq_length = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1436, in call  *\n            outputs = self.roberta(\n        File \"/usr/local/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileztupeatj.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/tmp/__autograph_generated_filee_elriq6.py\", line 76, in tf__call\n            batch_size, seq_length = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'roberta' (type TFXLMRobertaMainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 1424, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 822, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer 'roberta' (type TFXLMRobertaMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tfxlm_roberta_for_sequence_classification_2' (type TFXLMRobertaForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 512), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, None, 512), dtype=int64)'}\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation\n",
    "results = baseline_model.run_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Display results\n",
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c655d7d",
   "metadata": {},
   "source": [
    "## Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dc635",
   "metadata": {},
   "source": [
    "Investigating catastrophic forgetting more deeply by comparing the weight shifts between a pre-trained multilingual model and a retrained, English-only model could provide a more granular understanding of how language-specific information is lost during retraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_difference(pretrained_weights, retrained_weights):\n",
    "    differences = []\n",
    "    for pretrained, retrained in zip(pretrained_weights, retrained_weights):\n",
    "        diff = np.linalg.norm(pretrained - retrained)  # L2 norm for weight difference\n",
    "        differences.append(diff)\n",
    "    return differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a15b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve weights\n",
    "pretrained_weights = model.get_weights()  # Pretrained model weights\n",
    "retrained_weights = model.get_weights()   # Retrained model weights\n",
    "\n",
    "# Compute the differences between pre-trained and retrained model\n",
    "weight_differences = compute_weight_difference(pretrained_weights, retrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd3710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize or identify which layers show the most significant changes\n",
    "most_changed_layers = np.argsort(weight_differences)[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce577d2",
   "metadata": {},
   "source": [
    "3.\tVisualize the Results:\n",
    "\n",
    "o\tYou could create a heatmap or bar chart to visualize the magnitude of the changes in each layer.\n",
    "\n",
    "o\tLayer-wise comparison (e.g., attention heads, feed-forward layers, etc.) could reveal which part of the model \"forgot\" multilingual information the most.\n",
    "\n",
    "4.\tInterpretation of Results:\n",
    "\n",
    "o\tBy examining the most significantly changed layers, you could hypothesize which parts of the model are most sensitive to language-specific data. For example:\n",
    "\n",
    "\tAttention layers might \"forget\" multilingual information, especially if they were tuned heavily toward English sentence structure.\n",
    "\n",
    "\tEmbedding layers might change more dramatically as they encode language-specific token representations.\n",
    "\n",
    "o\tComparing only certain parts of the network, like the early transformer layers (closer to embeddings), versus the deeper layers (closer to output), could give insights into how the model's ability to generalize across languages is disrupted.\n",
    "\n",
    "________________________________________\n",
    "Challenges:\n",
    "\n",
    "•\tScale: Since models like XLM-Roberta are large (hundreds of millions of parameters), computing and storing weight differences across all layers could be resource-intensive.\n",
    "\n",
    "•\tLayer-specific analysis: Some layers (e.g., final layers or task-specific heads) may change more due to the retraining task (e.g., classification), so you’d want to focus on layers like attention and embeddings that are more involved in the multilingual representation.\n",
    "\n",
    "•\tLearning rate and retraining settings: Differences in fine-tuning settings (like learning rate) could also cause changes in specific parts of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade86b66",
   "metadata": {},
   "source": [
    "# 2 - Second result reproduction: \"better\" performance with adaptation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a99999",
   "metadata": {},
   "source": [
    "## Frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c58724",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6667a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 349.65 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 119.83 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 128.06 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 148.00 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 28.52 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully froze first 6 transformer layers.\n",
      "1/1 [==============================] - 21s 21s/step - loss: 0.6903 - auc_1: 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 03:56:40.414919: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'final_memory': 13611.21875,\n",
      " 'initial_memory': 9474.7578125,\n",
      " 'model_params': 278059797,\n",
      " 'results': {'de': {'LRAP': 0.39490411549235077,\n",
      "                    'Macro F1': 0.09523809523809523,\n",
      "                    'Micro F1': 0.4444444444444444,\n",
      "                    'R-Precision': 0.2},\n",
      "             'en': {'LRAP': 0.5909722222222222,\n",
      "                    'Macro F1': 0.07936507936507936,\n",
      "                    'Micro F1': 0.4615384615384615,\n",
      "                    'R-Precision': 0.30000000000000004},\n",
      "             'fi': {'LRAP': 0.23452380952380952,\n",
      "                    'Macro F1': 0.047619047619047616,\n",
      "                    'Micro F1': 0.28571428571428575,\n",
      "                    'R-Precision': 0.2},\n",
      "             'fr': {'LRAP': 0.1568890056022409,\n",
      "                    'Macro F1': 0.0,\n",
      "                    'Micro F1': 0.0,\n",
      "                    'R-Precision': 0.0}},\n",
      " 'training_time': 20.674821138381958}\n"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = frozen_model.run_training_pipeline_with_freezing(df=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs, n_frozen_layer= N)\n",
    "\n",
    "# Display results\n",
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc55bd8",
   "metadata": {},
   "source": [
    "## Adaptaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb9d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taining parameters\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:14<00:00, 335.83 examples/s]\n",
      "Map: 100%|██████████| 987/987 [00:05<00:00, 173.12 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:04<00:00, 212.96 examples/s]\n",
      "Map:   0%|          | 0/1003 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation of model with N frozen layers and same other parameters\n",
    "results = adapter_model.run_adapter_training_pipeline(data=df,train_sample_size=train_size,\n",
    "                                test_sample_size=test_size,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs)\n",
    "\n",
    "# Display results\n",
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
