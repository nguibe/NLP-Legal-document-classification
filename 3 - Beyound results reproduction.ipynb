{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f94e9b0",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part III: Performance improvement and pattern analysis\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed2c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61524bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('https://minio.lab.sspcloud.fr/nguibe/NLP/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab2041",
   "metadata": {},
   "source": [
    "# 1 - Original model through token analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8258577",
   "metadata": {},
   "source": [
    "This section was intended to explore token patterns accross languages in the specific legal field.\n",
    "\n",
    "Due to time constraints, this analysis was not completed. However, the following code sketch could be used to pursue this direction later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed48645",
   "metadata": {},
   "source": [
    "## Cleaning and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11157383",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download pl_core_news_sm\n",
    "!python -m spacy download fi_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee35895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models for each language\n",
    "spacy_models = {\n",
    "    \"en\": spacy.load(\"en_core_web_sm\"),\n",
    "    \"fr\": spacy.load(\"fr_core_news_sm\"),\n",
    "    \"de\": spacy.load(\"de_core_news_sm\"),\n",
    "    \"pl\": spacy.load(\"pl_core_news_sm\"),\n",
    "    \"fi\": spacy.load(\"fi_core_news_sm\")\n",
    "}\n",
    "\n",
    "# List of languages you care about\n",
    "languages = [\"en\", \"fr\", \"de\", \"pl\", \"fi\"]\n",
    "\n",
    "# Function to clean and lemmatize text\n",
    "def clean_and_lemmatize(text, lang_code, remove_stopwords=True):\n",
    "    if lang_code not in spacy_models:\n",
    "        return None\n",
    "    \n",
    "    nlp = spacy_models[lang_code]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if not token.is_punct and not token.is_space and (not token.is_stop if remove_stopwords else True)\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning/lemmatization across the dataframe\n",
    "def process_dataframe(df, languages):\n",
    "    lemmatized_texts = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text_dict = row[\"text\"]  # {lang: text}\n",
    "        labels = row[\"level_1_labels\"]\n",
    "\n",
    "        for lang in languages:\n",
    "            if isinstance(text_dict, dict) and lang in text_dict:\n",
    "                raw_text = text_dict[lang]\n",
    "                lemmatized = clean_and_lemmatize(raw_text, lang)\n",
    "                if lemmatized:\n",
    "                    lemmatized_texts.append({\n",
    "                        \"lang\": lang,\n",
    "                        \"text_lemmatized\": lemmatized,\n",
    "                        \"labels\": labels\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(lemmatized_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "processed_df = process_dataframe(df, languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "processed_df.to_parquet(\"data/processed_legal_texts.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22d80a",
   "metadata": {},
   "source": [
    "## Token - Label co-occurence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed lemmatized texts\n",
    "df = pd.read_parquet(\"data/processed_legal_texts.parquet\")\n",
    "\n",
    "# Build token-label co-occurrence mapping\n",
    "token_label_counts = defaultdict(Counter)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tokens = row[\"text_lemmatized\"].split()\n",
    "    labels = row[\"labels\"]\n",
    "    \n",
    "# Count each token against all its labels\n",
    "    for token in set(tokens):\n",
    "        for label in labels:\n",
    "            token_label_counts[token][label] += 1\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "top_tokens = sorted(token_label_counts.items(), key=lambda x: sum(x[1].values()), reverse=True)[:100]\n",
    "\n",
    "rows = []\n",
    "for token, label_counter in top_tokens:\n",
    "    for label, count in label_counter.items():\n",
    "        rows.append({\"token\": token, \"label\": label, \"count\": count})\n",
    "\n",
    "df_token_label = pd.DataFrame(rows)\n",
    "df_token_label.to_csv(\"output/token_label_cooccurrence.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token-label counts for one label\n",
    "def plot_top_tokens_for_label(label, top_n=10):\n",
    "    label_filtered = df_token_label[df_token_label[\"label\"] == label]\n",
    "    top = label_filtered.sort_values(\"count\", ascending=False).head(top_n)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(top[\"token\"], top[\"count\"])\n",
    "    plt.title(f\"Top {top_n} tokens for label {label}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac42346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: plot_top_tokens_for_label(\"xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd659b22",
   "metadata": {},
   "source": [
    "## Token distribution accross languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same preprocessed file\n",
    "df = pd.read_parquet(\"data/processed_legal_texts.parquet\")\n",
    "\n",
    "# Count token frequencies by language\n",
    "lang_token_freq = defaultdict(Counter)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    lang = row[\"lang\"]\n",
    "    tokens = row[\"text_lemmatized\"].split()\n",
    "    lang_token_freq[lang].update(tokens)\n",
    "\n",
    "# Create a DataFrame with token frequencies across languages\n",
    "def get_freq_df(top_tokens=None, min_freq=50):\n",
    "    all_tokens = set()\n",
    "    if top_tokens:\n",
    "        all_tokens = set(top_tokens)\n",
    "    else:\n",
    "        # Get common tokens across languages\n",
    "        for lang, counter in lang_token_freq.items():\n",
    "            common = {token for token, freq in counter.items() if freq > min_freq}\n",
    "            all_tokens |= common\n",
    "\n",
    "    data = []\n",
    "    for token in all_tokens:\n",
    "        row = {\"token\": token}\n",
    "        for lang in lang_token_freq:\n",
    "            row[lang] = lang_token_freq[lang][token]\n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of selected tokens\n",
    "def plot_token_distribution(tokens):\n",
    "    df_freq = get_freq_df(tokens)\n",
    "    df_freq.set_index(\"token\").T.plot(kind='bar', figsize=(10, 5))\n",
    "    plt.title(\"Token frequency across languages\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Language\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4739480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plot_token_distribution([\"regulation\", \"union\", \"market\", \"recht\", \"union\", \"protection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdba69b",
   "metadata": {},
   "source": [
    "# 2 - Other strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b707c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import label_embedding, prompt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cad947",
   "metadata": {},
   "source": [
    "## Prompt based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda5ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test set size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:05<00:00, 922.15 examples/s] \n",
      "Map: 100%|██████████| 987/987 [00:02<00:00, 469.74 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:01<00:00, 723.11 examples/s]\n",
      "Map: 100%|██████████| 1003/1003 [00:02<00:00, 494.85 examples/s]\n",
      "Map: 100%|██████████| 1014/1014 [00:02<00:00, 453.33 examples/s]\n",
      "Map: 100%|██████████| 972/972 [00:01<00:00, 488.07 examples/s]\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "157/157 [==============================] - 2396s 15s/step - loss: 0.3562 - auc_1: 0.4938\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 2331s 15s/step - loss: 0.3467 - auc_1: 0.4985\n",
      "Training time: 4727.13 seconds\n",
      "Initial memory usage: 14895.63 MB\n",
      "Final memory usage: 41035.94 MB\n",
      "Memory used during training: 26140.31 MB\n",
      "[INFO] Evaluating on language: de\n",
      "R-Precision: 0.2700\n",
      "Micro F1: 0.2289\n",
      "Macro F1: 0.0329\n",
      "LRAP: 0.5418\n",
      "Evaluation time: 161.53 seconds\n",
      "[INFO] Evaluating on language: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 14:52:19.536348: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Precision: 0.2688\n",
      "Micro F1: 0.2366\n",
      "Macro F1: 0.0335\n",
      "LRAP: 0.5420\n",
      "Evaluation time: 164.89 seconds\n",
      "[INFO] Evaluating on language: fi\n",
      "R-Precision: 0.2687\n",
      "Micro F1: 0.2309\n",
      "Macro F1: 0.0331\n",
      "LRAP: 0.5333\n",
      "Evaluation time: 164.10 seconds\n",
      "[INFO] Evaluating on language: fr\n",
      "R-Precision: 0.2740\n",
      "Micro F1: 0.2419\n",
      "Macro F1: 0.0339\n",
      "LRAP: 0.5532\n",
      "Evaluation time: 166.29 seconds\n",
      "[INFO] Evaluating on language: pl\n",
      "R-Precision: 0.2711\n",
      "Micro F1: 0.2399\n",
      "Macro F1: 0.0342\n",
      "LRAP: 0.5391\n",
      "Evaluation time: 158.41 seconds\n"
     ]
    }
   ],
   "source": [
    "results =prompt_model.run_prompt_classification(\n",
    "    df=df,\n",
    "    train_size=5000,\n",
    "    test_size=5000,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    prompt_type=\"guided\",  # or \"generic\"\n",
    "    # freeze_layers=6        # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb185280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': {'R-Precision': 0.27001013171225935,\n",
       "  'Micro F1': 0.22891036906854131,\n",
       "  'Macro F1': 0.032903877731463936,\n",
       "  'LRAP': 0.5418273646792338,\n",
       "  'Eval Time (s)': 161.53033113479614},\n",
       " 'en': {'R-Precision': 0.26884765625,\n",
       "  'Micro F1': 0.23659574468085104,\n",
       "  'Macro F1': 0.03351416515973478,\n",
       "  'LRAP': 0.5419704551780962,\n",
       "  'Eval Time (s)': 164.89226126670837},\n",
       " 'fi': {'R-Precision': 0.26869391824526423,\n",
       "  'Micro F1': 0.23091891891891894,\n",
       "  'Macro F1': 0.033088577005297895,\n",
       "  'LRAP': 0.5332799534823319,\n",
       "  'Eval Time (s)': 164.10067582130432},\n",
       " 'fr': {'R-Precision': 0.27396449704142006,\n",
       "  'Micro F1': 0.2419146183699871,\n",
       "  'Macro F1': 0.03392290249433107,\n",
       "  'LRAP': 0.5531542290513365,\n",
       "  'Eval Time (s)': 166.28934144973755},\n",
       " 'pl': {'R-Precision': 0.27109053497942387,\n",
       "  'Micro F1': 0.23991179713340685,\n",
       "  'Macro F1': 0.03417514763161201,\n",
       "  'LRAP': 0.5390594264784088,\n",
       "  'Eval Time (s)': 158.4082236289978}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c305c57",
   "metadata": {},
   "source": [
    "## label based embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "100%|██████████| 157/157 [45:19<00:00, 17.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTS]\n",
      "Language: en\n",
      "Top-5 Micro F1: 0.2631\n",
      "Top-5 Macro F1: 0.1001\n",
      "Top-5 LRAP:     0.2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "100%|██████████| 157/157 [35:48<00:00, 13.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTS]\n",
      "Language: fr\n",
      "Top-5 Micro F1: 0.2649\n",
      "Top-5 Macro F1: 0.0941\n",
      "Top-5 LRAP:     0.2174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "100%|██████████| 157/157 [27:45<00:00, 10.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTS]\n",
      "Language: de\n",
      "Top-5 Micro F1: 0.2637\n",
      "Top-5 Macro F1: 0.0923\n",
      "Top-5 LRAP:     0.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "100%|██████████| 157/157 [27:55<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTS]\n",
      "Language: pl\n",
      "Top-5 Micro F1: 0.2634\n",
      "Top-5 Macro F1: 0.0907\n",
      "Top-5 LRAP:     0.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "100%|██████████| 157/157 [27:22<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RESULTS]\n",
      "Language: fi\n",
      "Top-5 Micro F1: 0.2654\n",
      "Top-5 Macro F1: 0.0938\n",
      "Top-5 LRAP:     0.2176\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for lang in [\"en\", \"fr\", \"de\", \"pl\", \"fi\"]:\n",
    "    res = label_embedding.run_label_embedding_classification(df, top_k=5, batch_size=32, eval_lang=lang)\n",
    "    all_results.append(res)\n",
    "\n",
    "final_df = pd.DataFrame(all_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
