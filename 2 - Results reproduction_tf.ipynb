{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 06:30:59.300984: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 06:30:59.400575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745821859.511197  128579 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745821859.520609  128579 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745821859.599597  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599642  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599646  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599649  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-28 06:30:59.608366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5477d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NLP-Legal-document-classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNLP-Legal-document-classification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'NLP-Legal-document-classification'"
     ]
    }
   ],
   "source": [
    "os.chdir('NLP-Legal-document-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('data/dataset/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703bb25",
   "metadata": {},
   "source": [
    "# Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only level 3 labels\n",
    "df['level_3_labels'] = df['eurovoc_concepts'].apply(lambda d: d['level_3'] if 'level_3' in d else [])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb79671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     celex_id publication_date  \\\n",
      "0  32006D0213       2006-03-06   \n",
      "1  32003R1330       2003-07-25   \n",
      "2  32003R1786       2003-09-29   \n",
      "3  31985R2590       1985-09-13   \n",
      "4  31993R1103       1993-04-30   \n",
      "\n",
      "                                                text  \\\n",
      "0  COMMISSION DECISION\\nof 6 March 2006\\nestablis...   \n",
      "1  Commission Regulation (EC) No 1330/2003\\nof 25...   \n",
      "2  Council Regulation (EC) No 1786/2003\\nof 29 Se...   \n",
      "3  *****\\nCOMMISSION REGULATION (EEC) No 2590/85\\...   \n",
      "4  COMMISSION REGULATION (EEC) No 1103/93 of 30 A...   \n",
      "\n",
      "                                    eurovoc_concepts  split  \\\n",
      "0  {'all_levels': ['1706', '1826', '2754', '3690'...  train   \n",
      "1  {'all_levels': ['1117', '1118', '1605', '2635'...  train   \n",
      "2  {'all_levels': ['2173', '4854', '614', '797'],...  train   \n",
      "3  {'all_levels': ['1201', '1261', '5334', '755',...  train   \n",
      "4  {'all_levels': ['1309', '2159', '2192', '235',...  train   \n",
      "\n",
      "                                         doc_lengths  max_doc_length  \\\n",
      "0  {'de': 3302, 'en': 3233, 'fi': 3073.0, 'fr': 3...            3642   \n",
      "1  {'de': 1430, 'en': 1328, 'fi': 1366.0, 'fr': 1...            1437   \n",
      "2  {'de': 19641, 'en': 17741, 'fi': 17298.0, 'fr'...           19641   \n",
      "3  {'de': 2720, 'en': 2525, 'fi': 2527.0, 'fr': 2...            2720   \n",
      "4  {'de': 29436, 'en': 27992, 'fi': None, 'fr': 3...           32297   \n",
      "\n",
      "                                      level_3_labels  \n",
      "0                [1386, 2825, 138, 2475, 3879, 3641]  \n",
      "1                                 [1115, 2656, 1602]  \n",
      "2                             [614, 712, 1277, 2443]  \n",
      "3                      [2413, 712, 2477, 4488, 2443]  \n",
      "4  [539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...  \n"
     ]
    }
   ],
   "source": [
    "train_df = df[df['split']=='train']\n",
    "# English-only training set\n",
    "train_df.loc[:,'text'] = train_df[\"text\"].apply(lambda x: isinstance(x, dict) and x.get(\"en\"))\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5970fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     celex_id publication_date  \\\n",
      "0  32013R1390       2013-12-16   \n",
      "1  32015R0176       2015-02-05   \n",
      "2  32015R0596       2015-04-15   \n",
      "3  32015D0041       2014-12-17   \n",
      "4  32013D0785       2013-12-16   \n",
      "\n",
      "                                                text  \\\n",
      "0  RÈGLEMENT (UE) No 1390/2013 DU CONSEIL\\ndu 16 ...   \n",
      "1  RÈGLEMENT D'EXÉCUTION (UE) 2015/176 DE LA COMM...   \n",
      "2  RÈGLEMENT D'EXÉCUTION (UE) 2015/596 DE LA COMM...   \n",
      "3  DÉCISION (UE) 2015/41 DU PARLEMENT EUROPÉEN ET...   \n",
      "4  DÉCISION DU CONSEIL\\ndu 16 décembre 2013\\nrela...   \n",
      "\n",
      "                                    eurovoc_concepts split  \\\n",
      "0  {'all_levels': ['1085', '1474', '2329', '2556'...  test   \n",
      "1  {'all_levels': ['2749', '3173', '5573', '5898'...  test   \n",
      "2  {'all_levels': ['1239', '1318', '1878', '2972'...  test   \n",
      "3  {'all_levels': ['1361', '1647', '2543', '2910'...  test   \n",
      "4  {'all_levels': ['1474', '1819', '2329', '2556'...  test   \n",
      "\n",
      "                                         doc_lengths  max_doc_length  \\\n",
      "0  {'de': 3728, 'en': 3410, 'fi': 3364.0, 'fr': 3...            3728   \n",
      "1  {'de': 1847, 'en': 1790, 'fi': 1694.0, 'fr': 1...            1914   \n",
      "2  {'de': 3098, 'en': 2907, 'fi': 2907.0, 'fr': 3...            3111   \n",
      "3  {'de': 3238, 'en': 3041, 'fi': 2933.0, 'fr': 3...            3238   \n",
      "4  {'de': 3019, 'en': 2899, 'fi': 2712.0, 'fr': 2...            3019   \n",
      "\n",
      "                                      level_3_labels lang  \n",
      "0  [3461, 5424, 5283, 122, 2106, 4590, 913, 4040,...   fr  \n",
      "1  [13, 138, 5781, 5283, 122, 87, 2106, 914, 2200...   fr  \n",
      "2  [1329, 5283, 122, 3810, 6052, 2106, 2972, 4590...   fr  \n",
      "3  [5655, 1410, 5283, 5781, 5087, 122, 335, 2106,...   fr  \n",
      "4                            [3461, 4040, 311, 2476]   fr  \n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "test_df = df[df['split']=='test']\n",
    "\n",
    "# Test set in multiple languages\n",
    "test_langs = [\"fr\", \"de\", \"pl\",'fi'] \n",
    "test_dfs = []\n",
    "\n",
    "for lang in test_langs:\n",
    "    # Filter rows where the language exists in the text dictionary\n",
    "    df_lang = test_df[test_df[\"text\"].apply(lambda x: isinstance(x, dict) and lang in x)]\n",
    "    \n",
    "    # Now extract the respective language text, and add the 'lang' column\n",
    "    df_lang.loc[:,\"text\"] = df_lang[\"text\"].apply(lambda x: x[lang])  # Extract the language text\n",
    "    df_lang[\"lang\"] = lang  # Add a new column for language\n",
    "    \n",
    "    # Append to test_dfs\n",
    "    test_dfs.append(df_lang)\n",
    "\n",
    "# Combine the list of DataFrames into one (exploded test set)\n",
    "final_test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(final_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8ccdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128579/3383169465.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(train_df[\"level_3_labels\"])]\n"
     ]
    }
   ],
   "source": [
    "# Fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(df[\"level_3_labels\"])\n",
    "\n",
    "# Now safely transform\n",
    "train_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(train_df[\"level_3_labels\"])]\n",
    "final_test_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(final_test_df[\"level_3_labels\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca1fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active labels for row 5:\n",
      "Index: 90, Label: 1810\n",
      "Index: 173, Label: 2487\n",
      "Index: 270, Label: 3489\n",
      "Index: 338, Label: 4314\n",
      "Index: 450, Label: 614\n"
     ]
    }
   ],
   "source": [
    "row_index = 5  # Change to the row you want to inspect\n",
    "label_vector = train_df[\"label_vector\"].iloc[row_index]\n",
    "\n",
    "active_labels = [\n",
    "    (i, mlb.classes_[i]) for i, val in enumerate(label_vector) if val == 1\n",
    "]\n",
    "\n",
    "print(f\"Active labels for row {row_index}:\")\n",
    "for idx, label in active_labels:\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f08371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test set with label 1810: [32, 181, 381, 480, 619, 708, 881, 940, 964, 1338, 1412, 1807, 2484, 2921, 3125, 3203, 3603, 3620, 3860, 4167, 4491, 4911, 4949, 5028, 5177, 5377, 5476, 5615, 5704, 5877, 5936, 5960, 6334, 6408, 6803, 7480, 7917, 8121, 8199, 8599, 8616, 8856, 9163, 9487, 9907, 9945, 10024, 10173, 10373, 10472, 10611, 10700, 10873, 10932, 10956, 11330, 11404, 11799, 12476, 12913, 13117, 13195, 13595, 13612, 13852, 14159, 14483, 14903, 14941, 15020, 15169, 15369, 15468, 15607, 15696, 15869, 15928, 15952, 16326, 16400, 16795, 17472, 17909, 18113, 18191, 18591, 18608, 18848, 19155, 19479, 19899, 19937]\n"
     ]
    }
   ],
   "source": [
    "# check same index for same label in test dataset\n",
    "label_id = \"1810\"\n",
    "label_index = list(mlb.classes_).index(label_id)\n",
    "\n",
    "# Get row indices in the test set where label 1810 is present\n",
    "matching_indices = [\n",
    "    i for i, row in enumerate(final_test_df[\"label_vector\"]) if row[label_index] == 1\n",
    "]\n",
    "\n",
    "print(f\"Rows in test set with label {label_id}: {matching_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0dc64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active labels for row 32:\n",
      "Index: 90, Label: 1810\n",
      "Index: 128, Label: 2188\n",
      "Index: 265, Label: 3461\n"
     ]
    }
   ],
   "source": [
    "row_index = 32  # Change to the row you want to inspect\n",
    "label_vector = final_test_df[\"label_vector\"].iloc[row_index]\n",
    "\n",
    "active_labels = [\n",
    "    (i, mlb.classes_[i]) for i, val in enumerate(label_vector) if val == 1\n",
    "]\n",
    "\n",
    "print(f\"Active labels for row {row_index}:\")\n",
    "for idx, label in active_labels:\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2235c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label_vector\"]])\n",
    "test_datasets = {\n",
    "    lang: Dataset.from_pandas(df[[\"text\", \"label_vector\"]]) \n",
    "    for lang, df in final_test_df.groupby(\"lang\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  71%|███████   | 39000/54994 [02:54<01:06, 241.52 examples/s]"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "def tokenize_and_format_tf(batch):\n",
    "    encodings = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    encodings['labels'] = batch['label_vector']\n",
    "    return encodings\n",
    "\n",
    "# Tokenize the datasets (train and test)\n",
    "train_dataset = train_dataset.map(tokenize_and_format_tf, batched=True)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(tokenize_and_format_tf, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9605ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to TensorFlow Dataset\n",
    "def dataset_to_tf(dataset):\n",
    "    def gen():\n",
    "        for example in dataset:\n",
    "            yield {\n",
    "                \"input_ids\": example['input_ids'],\n",
    "                \"attention_mask\": example['attention_mask']\n",
    "            }, example['labels']\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(512,), dtype=tf.int64),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(512,), dtype=tf.int64)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Convert both train and test datasets to TensorFlow Dataset\n",
    "train_tf_dataset = dataset_to_tf(train_dataset)\n",
    "test_tf_datasets = {lang: dataset_to_tf(test_datasets[lang]) for lang in test_datasets}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5926c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for multi-label classification\n",
    "# Get the number of labels\n",
    "num_labels = len(mlb.classes_)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=num_labels, problem_type='multi_label_classification'\n",
    ")\n",
    "\n",
    "# Compile the model with appropriate loss and optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_tf_dataset.batch(8), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f995c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model for each language\n",
    "for lang, tf_dataset in test_tf_datasets.items():\n",
    "    results = model.evaluate(tf_dataset.batch(8))\n",
    "    print(f\"Language: {lang}\")\n",
    "    print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "767b6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95073/2203363418.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m     18\u001b[39m trainer = Trainer(\n\u001b[32m     19\u001b[39m     model=model,\n\u001b[32m     20\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1368\u001b[39m, in \u001b[36mXLMRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1367\u001b[39m         loss_fct = BCEWithLogitsLoss()\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m         loss = \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1371\u001b[39m     output = (logits,) + outputs[\u001b[32m2\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:3643\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m   3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3644\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n\u001b[32m   3645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm-roberta-eurovoc\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_dir=\"./logs\",                    # Log directory\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_datasets[\"fr\"],  # Or \"de\", \"es\" — you can loop through them too\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
