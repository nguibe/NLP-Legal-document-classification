{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('data/dataset/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a65285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_to_keep = ['en', 'de', 'fr', 'pl', 'fi'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43cc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6b7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of the document for each language\n",
    "def compute_lengths(text_dict):\n",
    "    lengths = {lang: len(text_dict[lang]) for lang in langs_to_keep if text_dict.get(lang) is not None}\n",
    "    return lengths\n",
    "# Apply the function to the 'text' column and store the result in a new column 'doc_lengths'\n",
    "df_reduced['doc_lengths'] = df_reduced['text'].apply(compute_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4300639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['max_doc_length'] = df_reduced['doc_lengths'].apply(lambda d: max(d.values(), default=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0643cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_reduced[df_reduced['max_doc_length']<500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21bc8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64990, 64990)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_reduced[df_reduced['max_doc_length']<500000]), len(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc497985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_parquet('data/dataset/multi_eurlex_reduced.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703bb25",
   "metadata": {},
   "source": [
    "# Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3e076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only level 3 labels\n",
    "df_reduced['level_3_labels'] = df_reduced['eurovoc_concepts'].apply(lambda d: d['level_3'] if 'level_3' in d else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a9b7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>celex_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>text</th>\n",
       "      <th>eurovoc_concepts</th>\n",
       "      <th>split</th>\n",
       "      <th>doc_lengths</th>\n",
       "      <th>max_doc_length</th>\n",
       "      <th>level_3_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32006D0213</td>\n",
       "      <td>2006-03-06</td>\n",
       "      <td>{'de': 'ENTSCHEIDUNG DER KOMMISSION\n",
       "vom 6. Mär...</td>\n",
       "      <td>{'all_levels': ['1706', '1826', '2754', '3690'...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 3233, 'de': 3302, 'fr': 3642, 'pl': 332...</td>\n",
       "      <td>3642</td>\n",
       "      <td>[1386, 2825, 138, 2475, 3879, 3641]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32003R1330</td>\n",
       "      <td>2003-07-25</td>\n",
       "      <td>{'de': 'Verordnung (EG) Nr. 1330/2003 der Komm...</td>\n",
       "      <td>{'all_levels': ['1117', '1118', '1605', '2635'...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 1328, 'de': 1430, 'fr': 1437, 'fi': 1366}</td>\n",
       "      <td>1437</td>\n",
       "      <td>[1115, 2656, 1602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32003R1786</td>\n",
       "      <td>2003-09-29</td>\n",
       "      <td>{'de': 'Verordnung (EG) Nr. 1786/2003 des Rate...</td>\n",
       "      <td>{'all_levels': ['2173', '4854', '614', '797'],...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 17741, 'de': 19641, 'fr': 19133, 'pl': ...</td>\n",
       "      <td>19641</td>\n",
       "      <td>[614, 712, 1277, 2443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31985R2590</td>\n",
       "      <td>1985-09-13</td>\n",
       "      <td>{'de': '*****\n",
       "VERORDNUNG (EWG) Nr. 2590/85 DER...</td>\n",
       "      <td>{'all_levels': ['1201', '1261', '5334', '755',...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 2525, 'de': 2720, 'fr': 2684, 'fi': 2527}</td>\n",
       "      <td>2720</td>\n",
       "      <td>[2413, 712, 2477, 4488, 2443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31993R1103</td>\n",
       "      <td>1993-04-30</td>\n",
       "      <td>{'de': 'VERORDNUNG (EWG) Nr. 1103/93 DER KOMMI...</td>\n",
       "      <td>{'all_levels': ['1309', '2159', '2192', '235',...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 27992, 'de': 29436, 'fr': 32297}</td>\n",
       "      <td>32297</td>\n",
       "      <td>[539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     celex_id publication_date  \\\n",
       "0  32006D0213       2006-03-06   \n",
       "1  32003R1330       2003-07-25   \n",
       "2  32003R1786       2003-09-29   \n",
       "3  31985R2590       1985-09-13   \n",
       "4  31993R1103       1993-04-30   \n",
       "\n",
       "                                                text  \\\n",
       "0  {'de': 'ENTSCHEIDUNG DER KOMMISSION\n",
       "vom 6. Mär...   \n",
       "1  {'de': 'Verordnung (EG) Nr. 1330/2003 der Komm...   \n",
       "2  {'de': 'Verordnung (EG) Nr. 1786/2003 des Rate...   \n",
       "3  {'de': '*****\n",
       "VERORDNUNG (EWG) Nr. 2590/85 DER...   \n",
       "4  {'de': 'VERORDNUNG (EWG) Nr. 1103/93 DER KOMMI...   \n",
       "\n",
       "                                    eurovoc_concepts  split  \\\n",
       "0  {'all_levels': ['1706', '1826', '2754', '3690'...  train   \n",
       "1  {'all_levels': ['1117', '1118', '1605', '2635'...  train   \n",
       "2  {'all_levels': ['2173', '4854', '614', '797'],...  train   \n",
       "3  {'all_levels': ['1201', '1261', '5334', '755',...  train   \n",
       "4  {'all_levels': ['1309', '2159', '2192', '235',...  train   \n",
       "\n",
       "                                         doc_lengths  max_doc_length  \\\n",
       "0  {'en': 3233, 'de': 3302, 'fr': 3642, 'pl': 332...            3642   \n",
       "1   {'en': 1328, 'de': 1430, 'fr': 1437, 'fi': 1366}            1437   \n",
       "2  {'en': 17741, 'de': 19641, 'fr': 19133, 'pl': ...           19641   \n",
       "3   {'en': 2525, 'de': 2720, 'fr': 2684, 'fi': 2527}            2720   \n",
       "4            {'en': 27992, 'de': 29436, 'fr': 32297}           32297   \n",
       "\n",
       "                                      level_3_labels  \n",
       "0                [1386, 2825, 138, 2475, 3879, 3641]  \n",
       "1                                 [1115, 2656, 1602]  \n",
       "2                             [614, 712, 1277, 2443]  \n",
       "3                      [2413, 712, 2477, 4488, 2443]  \n",
       "4  [539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb79671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_reduced[df_reduced['split']=='train']\n",
    "# English-only training set\n",
    "train_df.loc[:,'text'] = train_df[\"text\"].apply(lambda x: isinstance(x, dict) and x.get(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5970fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test_df = df_reduced[df_reduced['split']=='test']\n",
    "\n",
    "# Test set in multiple languages\n",
    "test_langs = [\"fr\", \"de\", \"pl\",'fi']  # whatever languages you want\n",
    "test_dfs = []\n",
    "\n",
    "for lang in test_langs:\n",
    "    # Filter rows where the language exists in the text dictionary\n",
    "    df_lang = test_df[test_df[\"text\"].apply(lambda x: isinstance(x, dict) and lang in x)]\n",
    "    \n",
    "    # Now extract the respective language text, and add the 'lang' column\n",
    "    df_lang.loc[:,\"text\"] = df_lang[\"text\"].apply(lambda x: x[lang])  # Extract the language text\n",
    "    df_lang[\"lang\"] = lang  # Add a new column for language\n",
    "    \n",
    "    # Append to test_dfs\n",
    "    test_dfs.append(df_lang)\n",
    "\n",
    "# Combine the list of DataFrames into one (exploded test set)\n",
    "final_test_df = pd.concat(test_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9326a71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>celex_id</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>text</th>\n",
       "      <th>eurovoc_concepts</th>\n",
       "      <th>split</th>\n",
       "      <th>doc_lengths</th>\n",
       "      <th>max_doc_length</th>\n",
       "      <th>level_3_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32006D0213</td>\n",
       "      <td>2006-03-06</td>\n",
       "      <td>COMMISSION DECISION\\nof 6 March 2006\\nestablis...</td>\n",
       "      <td>{'all_levels': ['1706', '1826', '2754', '3690'...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 3233, 'de': 3302, 'fr': 3642, 'pl': 332...</td>\n",
       "      <td>3642</td>\n",
       "      <td>[1386, 2825, 138, 2475, 3879, 3641]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32003R1330</td>\n",
       "      <td>2003-07-25</td>\n",
       "      <td>Commission Regulation (EC) No 1330/2003\\nof 25...</td>\n",
       "      <td>{'all_levels': ['1117', '1118', '1605', '2635'...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 1328, 'de': 1430, 'fr': 1437, 'fi': 1366}</td>\n",
       "      <td>1437</td>\n",
       "      <td>[1115, 2656, 1602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32003R1786</td>\n",
       "      <td>2003-09-29</td>\n",
       "      <td>Council Regulation (EC) No 1786/2003\\nof 29 Se...</td>\n",
       "      <td>{'all_levels': ['2173', '4854', '614', '797'],...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 17741, 'de': 19641, 'fr': 19133, 'pl': ...</td>\n",
       "      <td>19641</td>\n",
       "      <td>[614, 712, 1277, 2443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31985R2590</td>\n",
       "      <td>1985-09-13</td>\n",
       "      <td>*****\\nCOMMISSION REGULATION (EEC) No 2590/85\\...</td>\n",
       "      <td>{'all_levels': ['1201', '1261', '5334', '755',...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 2525, 'de': 2720, 'fr': 2684, 'fi': 2527}</td>\n",
       "      <td>2720</td>\n",
       "      <td>[2413, 712, 2477, 4488, 2443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31993R1103</td>\n",
       "      <td>1993-04-30</td>\n",
       "      <td>COMMISSION REGULATION (EEC) No 1103/93 of 30 A...</td>\n",
       "      <td>{'all_levels': ['1309', '2159', '2192', '235',...</td>\n",
       "      <td>train</td>\n",
       "      <td>{'en': 27992, 'de': 29436, 'fr': 32297}</td>\n",
       "      <td>32297</td>\n",
       "      <td>[539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     celex_id publication_date  \\\n",
       "0  32006D0213       2006-03-06   \n",
       "1  32003R1330       2003-07-25   \n",
       "2  32003R1786       2003-09-29   \n",
       "3  31985R2590       1985-09-13   \n",
       "4  31993R1103       1993-04-30   \n",
       "\n",
       "                                                text  \\\n",
       "0  COMMISSION DECISION\\nof 6 March 2006\\nestablis...   \n",
       "1  Commission Regulation (EC) No 1330/2003\\nof 25...   \n",
       "2  Council Regulation (EC) No 1786/2003\\nof 29 Se...   \n",
       "3  *****\\nCOMMISSION REGULATION (EEC) No 2590/85\\...   \n",
       "4  COMMISSION REGULATION (EEC) No 1103/93 of 30 A...   \n",
       "\n",
       "                                    eurovoc_concepts  split  \\\n",
       "0  {'all_levels': ['1706', '1826', '2754', '3690'...  train   \n",
       "1  {'all_levels': ['1117', '1118', '1605', '2635'...  train   \n",
       "2  {'all_levels': ['2173', '4854', '614', '797'],...  train   \n",
       "3  {'all_levels': ['1201', '1261', '5334', '755',...  train   \n",
       "4  {'all_levels': ['1309', '2159', '2192', '235',...  train   \n",
       "\n",
       "                                         doc_lengths  max_doc_length  \\\n",
       "0  {'en': 3233, 'de': 3302, 'fr': 3642, 'pl': 332...            3642   \n",
       "1   {'en': 1328, 'de': 1430, 'fr': 1437, 'fi': 1366}            1437   \n",
       "2  {'en': 17741, 'de': 19641, 'fr': 19133, 'pl': ...           19641   \n",
       "3   {'en': 2525, 'de': 2720, 'fr': 2684, 'fi': 2527}            2720   \n",
       "4            {'en': 27992, 'de': 29436, 'fr': 32297}           32297   \n",
       "\n",
       "                                      level_3_labels  \n",
       "0                [1386, 2825, 138, 2475, 3879, 3641]  \n",
       "1                                 [1115, 2656, 1602]  \n",
       "2                             [614, 712, 1277, 2443]  \n",
       "3                      [2413, 712, 2477, 4488, 2443]  \n",
       "4  [539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3827b509",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_label_matrix = \u001b[43mmlb\u001b[49m.transform(train_df[\u001b[33m\"\u001b[39m\u001b[33mlevel_3_labels\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Check shape consistency\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m train_label_matrix.shape[\u001b[32m0\u001b[39m] == train_df.shape[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mMismatch in rows!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'mlb' is not defined"
     ]
    }
   ],
   "source": [
    "train_label_matrix = mlb.transform(train_df[\"level_3_labels\"])\n",
    "\n",
    "# Check shape consistency\n",
    "assert train_label_matrix.shape[0] == train_df.shape[0], \"Mismatch in rows!\"\n",
    "\n",
    "# Now assign safely\n",
    "train_df = train_df.copy()  # Avoid SettingWithCopyWarning\n",
    "train_df[\"label_vector\"] = [row.tolist() for row in train_label_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52928ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95073/400139628.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"label_vector\"] = [row.tolist() for row in label_matrix]\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['41', '6138', '7815', '7817', '7855', '7861', '7868', '7879', '8301', '962'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "label_matrix = mlb.fit_transform(train_df[\"level_3_labels\"])\n",
    "train_df[\"label_vector\"] = [row.tolist() for row in label_matrix]\n",
    "\n",
    "# Apply same transformation to test sets\n",
    "final_test_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(final_test_df[\"level_3_labels\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b60f003e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label_vector\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2235c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label_vector\"]])\n",
    "test_datasets = {\n",
    "    lang: Dataset.from_pandas(df[[\"text\", \"label_vector\"]]) \n",
    "    for lang, df in final_test_df.groupby(\"lang\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3541637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:42<00:00, 1279.24 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:09<00:00, 537.85 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:08<00:00, 609.07 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:09<00:00, 518.93 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:09<00:00, 525.25 examples/s]\n",
      "Map: 100%|██████████| 54994/54994 [00:19<00:00, 2775.05 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 3530.64 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 2505.32 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 2523.33 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 2500.61 examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     test_datasets[lang] = test_datasets[lang].map(prepare_dataset)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Set format for input_ids and attention_mask (int64) for both train and test datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m train_dataset.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m], dtype=\u001b[43mtorch\u001b[49m.int64)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Set format for labels (float32) for multi-label classification\u001b[39;00m\n\u001b[32m     25\u001b[39m train_dataset.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenization function (ensure it handles both text and labels correctly)\n",
    "def tokenize(batch):\n",
    "    if isinstance(batch[\"text\"], list):\n",
    "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    else:\n",
    "        texts = [str(item) for item in batch[\"text\"]]  # Ensure the text is in the correct format\n",
    "        return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(tokenize, batched=True)\n",
    "\n",
    "# Prepare labels (float32 for multi-label classification)\n",
    "def prepare_dataset(example):\n",
    "    example[\"labels\"] = example[\"label_vector\"]  # Your label_vector is the multi-label encoding\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(prepare_dataset)\n",
    "\n",
    "# Set format for input_ids and attention_mask (int64) for both train and test datasets\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], dtype=torch.int64)\n",
    "# Set format for labels (float32) for multi-label classification\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"labels\"], dtype=torch.float32)\n",
    "\n",
    "# Do the same for test datasets\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], dtype=torch.int64)\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"labels\"], dtype=torch.float32)\n",
    "\n",
    "print(train_dataset[0])  # Check the first example after format adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a491578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4877cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Set format for input_ids and attention_mask (int64) for both train and test datasets\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], dtype=torch.int64)\n",
    "# Set format for labels (float32) for multi-label classification\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"labels\"], dtype=torch.float32)\n",
    "\n",
    "# Do the same for test datasets\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], dtype=torch.int64)\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"labels\"], dtype=torch.float32)\n",
    "\n",
    "print(train_dataset[0])  # Check the first example after format adjustment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d0bba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(mlb.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    problem_type=\"multi_label_classification\",  # Ensure this is correct for multi-label task\n",
    "    num_labels=num_labels,  # The number of labels from the MultiLabelBinarizer\n",
    "    id2label={i: label for i, label in enumerate(mlb.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(mlb.classes_)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "767b6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95073/2203363418.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m     18\u001b[39m trainer = Trainer(\n\u001b[32m     19\u001b[39m     model=model,\n\u001b[32m     20\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1368\u001b[39m, in \u001b[36mXLMRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1367\u001b[39m         loss_fct = BCEWithLogitsLoss()\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m         loss = \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1371\u001b[39m     output = (logits,) + outputs[\u001b[32m2\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:3643\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m   3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3644\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n\u001b[32m   3645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm-roberta-eurovoc\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_dir=\"./logs\",                    # Log directory\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_datasets[\"fr\"],  # Or \"de\", \"es\" — you can loop through them too\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb66a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5148c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [01:02<00:00, 873.04 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:12<00:00, 397.19 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:11<00:00, 424.15 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:12<00:00, 394.05 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:13<00:00, 369.92 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_format(batch):\n",
    "    # Tokenize the texts (make sure to include padding and truncation)\n",
    "    encodings = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    # Add labels to the encoded data\n",
    "    encodings['labels'] = batch['label_vector']\n",
    "    return encodings\n",
    "\n",
    "# Tokenize and add labels to the training and test datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_format, batched=True)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(tokenize_and_format, batched=True)\n",
    "    \n",
    "# Check if all the required columns are present in train and test datasets\n",
    "print(train_dataset[0])  # Should show input_ids, attention_mask, and labels\n",
    "\n",
    "# Now set the format correctly:\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], dtype=torch.int64)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bbcf6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__'],\n",
      "    num_rows: 54994\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2b36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Make sure batch[\"text\"] is a list of strings, not a list of dictionaries\n",
    "    if isinstance(batch[\"text\"], list):\n",
    "        # If already a list of strings, continue\n",
    "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    else:\n",
    "        # If it's not, extract the correct string from each entry (e.g., handling dicts)\n",
    "        texts = [str(item) for item in batch[\"text\"]]  # Convert each item to string (adjust if it's a dictionary)\n",
    "        return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db29ebb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/tokenizer/tokenizer_config.json',\n",
       " 'model/tokenizer/special_tokens_map.json',\n",
       " 'model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer to a specified directory\n",
    "tokenizer.save_pretrained(\"model/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bebddc51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in model/tokenizer/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the tokenizer from the local directory\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel/tokenizer/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:966\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    964\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1151\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[32m   1149\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern].from_dict(config_dict, **unused_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1152\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1153\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, or contain one of the following strings \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1154\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(CONFIG_MAPPING.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1155\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized model in model/tokenizer/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ea2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:49<00:00, 1106.69 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:11<00:00, 444.51 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:10<00:00, 487.82 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:11<00:00, 416.63 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:11<00:00, 426.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to the training dataset\n",
    "def tokenize(batch):\n",
    "    # batch[\"text\"] is already List[str]\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Apply tokenization to each language-specific test dataset\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97ddb8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 54994\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# still not empty?\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1df7da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'COMMISSION DECISION\\nof 6 March 2006\\nestablishing the classes of reaction-to-fire performance for certain construction products as regards wood flooring and solid wood panelling and cladding\\n(notified under document number C(2006) 655)\\n(Text with EEA relevance)\\n(2006/213/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Directive 89/106/EEC of 21 December 1988, on the approximation of laws, regulations and administrative provisions of the Member States relating to construction products (1), and in particular Article 20(2) thereof,\\nWhereas:\\n(1)\\nDirective 89/106/EEC envisages that in order to take account of different levels of protection for construction works at national, regional or local level, it may be necessary to establish in the interpretative documents classes corresponding to the performance of products in respect of each essential requirement. Those documents have been published as the ‘Communication of the Commission with regard to the interpretative documents of Directive 89/106/EEC’ (2).\\n(2)\\nWith respect to the essential requirement of safety in the event of fire, interpretative document No 2 lists a number of interrelated measures which together define the fire safety strategy to be variously developed in the Member States.\\n(3)\\nInterpretative document No 2 identifies one of those measures as the limitation of the generation and spread of fire and smoke within a given area by limiting the potential of construction products to contribute to the full development of a fire.\\n(4)\\nThe level of that limitation may be expressed only in terms of the different levels of reaction-to-fire performance of the products in their end-use application;\\n(5)\\nBy way of harmonised solution, a system of classes was adopted in Commission Decision 2000/147/EC of 8 February 2000 implementing Council Directive 89/106/EEC as regards the classification of the reaction-to-fire performance of construction products (3).\\n(6)\\nIn the case of wood flooring and solid wood panelling and cladding it is necessary to use the classification established in Decision 2000/147/EC.\\n(7)\\nThe reaction-to-fire performance of many construction products and/or materials, within the classification provided for in Decision 2000/147/EC, is well established and sufficiently well known to fire regulators in Member States that they do not require testing for this particular performance characteristic.\\n(8)\\nThe measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Construction,\\nHAS ADOPTED THIS DECISION:\\nArticle 1\\nThe construction products and/or materials which satisfy all the requirements of the performance characteristic ‘reaction to fire’ without need for further testing are set out in the Annex.\\nArticle 2\\nThe specific classes to be applied to different construction products and/or materials, within the reaction-to-fire classification adopted in Decision 2000/147/EC, are set out in the Annex to this Decision.\\nArticle 3\\nProducts shall be considered in relation to their end-use application, where relevant.\\nArticle 4\\nThis Decision is addressed to the Member States.\\nDone at Brussels, 6 March 2006.', 'label_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '__index_level_0__': 0, 'input_ids': [0, 31436, 230498, 1514, 17304, 118596, 111, 305, 11994, 3295, 137633, 214, 70, 61112, 111, 132539, 9, 188, 9, 73702, 23718, 100, 24233, 50961, 38742, 237, 28601, 7, 109412, 74912, 214, 136, 18652, 109412, 16138, 2069, 136, 26580, 59725, 15, 10869, 47314, 1379, 12937, 14012, 313, 132, 22435, 16, 305, 70265, 15, 174379, 678, 241, 24040, 89515, 329, 16, 15, 22435, 12477, 2681, 64, 16546, 16, 23373, 31436, 230498, 31766, 23373, 206713, 9330, 31436, 594, 65761, 179904, 4, 171855, 28601, 47, 70, 4804, 67530, 137633, 214, 70, 28811, 66069, 4, 171855, 28601, 47, 42666, 5844, 17853, 64, 54100, 64, 24821, 441, 111, 952, 14487, 18592, 4, 98, 70, 35707, 53950, 2320, 111, 131703, 4, 209332, 136, 86757, 125034, 7, 111, 70, 74057, 46684, 33444, 214, 47, 50961, 38742, 798, 4, 136, 23, 17311, 24241, 387, 40970, 2685, 4390, 4, 78662, 162, 12, 798, 42666, 5844, 17853, 64, 54100, 64, 24821, 441, 6, 191195, 7, 450, 23, 12989, 47, 5646, 15426, 111, 12921, 90926, 111, 48431, 100, 50961, 43240, 99, 15889, 4, 18150, 707, 4000, 17366, 4, 442, 1543, 186, 63559, 47, 137633, 23, 70, 26389, 4935, 60525, 61112, 42518, 214, 47, 70, 23718, 111, 38742, 23, 15072, 111, 12638, 85590, 64209, 674, 5, 139661, 60525, 765, 2809, 91376, 237, 70, 204, 166697, 14, 120639, 111, 70, 63871, 678, 28601, 47, 70, 26389, 4935, 60525, 111, 42666, 5844, 17853, 64, 54100, 64, 24821, 441, 26, 1737, 5, 1737, 17106, 15072, 47, 70, 85590, 64209, 674, 111, 81900, 23, 70, 19732, 111, 11476, 4, 26389, 4935, 12937, 438, 116, 5303, 7, 10, 14012, 111, 1940, 174822, 72350, 7, 3129, 25842, 61924, 70, 11476, 81900, 113857, 47, 186, 67842, 538, 126809, 23, 70, 74057, 46684, 5, 2788, 159838, 4935, 12937, 438, 116, 6, 42485, 90, 1632, 111, 8382, 72350, 7, 237, 70, 205969, 111, 70, 58093, 136, 93403, 111, 11476, 136, 2614, 350, 28032, 10, 34475, 16128, 390, 17475, 214, 70, 38516, 111, 50961, 38742, 47, 162466, 47, 70, 4393, 34754, 111, 10, 11476, 5, 3971, 581, 17366, 111, 450, 205969, 1543, 186, 36510, 297, 4734, 23, 69407, 111, 70, 12921, 90926, 111, 132539, 9, 188, 9, 73702, 23718, 111, 70, 38742, 23, 2363, 3564, 9, 4032, 38415, 74, 5211, 3311, 3917, 111, 22313, 5281, 29806, 4, 10, 5426, 111, 61112, 509, 30666, 297, 23, 63871, 49132, 6889, 3576, 64, 128497, 64, 16546, 111, 382, 22482, 3576, 29479, 214, 70615, 42666, 5844, 17853, 64, 54100, 64, 24821, 441, 237, 28601, 7, 70, 40865, 1363, 111, 70, 132539, 9, 188, 9, 73702, 23718, 111, 50961, 38742, 2788, 5, 6791, 360, 70, 7225, 111, 109412, 74912, 214, 136, 18652, 109412, 16138, 2069, 136, 26580, 59725, 442, 83, 63559, 47, 4527, 70, 40865, 1363, 170920, 23, 49132, 6889, 3576, 64, 128497, 64, 16546, 5, 8696, 581, 132539, 9, 188, 9, 73702, 23718, 111, 5941, 50961, 38742, 136, 64, 748, 76319, 4, 28032, 70, 40865, 1363, 62952, 100, 23, 49132, 6889, 3576, 64, 128497, 64, 16546, 4, 83, 5299, 170920, 136, 6, 129980, 538, 5299, 51529, 47, 11476, 144314, 7, 23, 74057, 46684, 450, 1836, 54, 959, 64209, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cefc1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'de': Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4996\n",
      "}), 'fi': Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4996\n",
      "}), 'fr': Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4996\n",
      "}), 'pl': Dataset({\n",
      "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4996\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28878bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VERORDNUNG (EU) Nr. 1390/2013 DES RATES\\nvom 16. Dezember 2013\\nüber die Aufteilung der Fangmöglichkeiten nach dem zwischen der Europäischen Union und der Union der Komoren vereinbarten Protokoll zur Festlegung der Fangmöglichkeiten und der finanziellen Gegenleistung nach dem partnerschaftlichen Fischereiabkommen zwischen den beiden Vertragsparteien\\nDER RAT DER EUROPÄISCHEN UNION -\\ngestützt auf den Vertrag über die Arbeitsweise der Europäischen Union, insbesondere auf Artikel 43 Absatz 3,\\nauf Vorschlag der Europäischen Kommission,\\nin Erwägung nachstehender Gründe:\\n(1)\\nAm 5. Oktober 2006 hat der Rat die Verordnung (EG) Nr. 1563/2006 (1) über den Abschluss des partnerschaftlichen Fischereiabkommens zwischen der Europäischen Gemeinschaft und der Union der Komoren (im Folgenden „partnerschaftliches Fischereiabkommen“) angenommen.\\n(2)\\nDie Europäische Union hat mit der Union der Komoren ein neues Protokoll zum partnerschaftlichen Fischereiabkommen (nachstehend „neues Protokoll“) ausgehandelt, mit dem den Fischereifahrzeugen der Europäischen Union Fangmöglichkeiten in den komorischen Gewässern eingeräumt werden. Nach Abschluss der Verhandlungen wurde am 5. Juli 2013 ein neues Protokoll paraphiert.\\n(3)\\nAm 16. Dezember 2013 hat der Rat den Beschluss 2013/786/EU (2) über die Unterzeichnung und vorläufige Anwendung des neuen Protokolls angenommen.\\n(4)\\nEs ist angezeigt, die Fangmöglichkeiten für die Anwendungsdauer des neuen Protokolls auf die Mitgliedstaaten aufzuteilen.\\n(5)\\nStellt sich heraus, dass die der Europäischen Union im Rahmen des neuen Protokolls eingeräumten Fangmöglichkeiten nicht vollständig ausgeschöpft werden, so unterrichtet die Kommission gemäß der Verordnung (EG) Nr. 1006/2008 des Rates (3) die betreffenden Mitgliedstaaten hierüber. Geht innerhalb einer vom Rat festzulegenden Frist keine Antwort ein, so gilt dies als Bestätigung, dass die Schiffe des betreffenden Mitgliedstaats ihre Fangmöglichkeiten in dem betreffenden Zeitraum nicht voll in Anspruch nehmen. Diese Frist sollte festgelegt werden.\\n(6)\\nDamit die Fischereifahrzeuge der Europäischen Union ihre Fangtätigkeiten nicht unterbrechen müssen, sieht das neue Protokoll dessen vorläufige Anwendung durch die Vertragsparteien ab dem 1. Januar 2014 vor. Vorliegende Verordnung sollte daher bereits ab der vorläufigen Anwendung des neuen Protokolls gelten -\\nHAT FOLGENDE VERORDNUNG ERLASSEN:\\nArtikel 1\\n(1) Die in dem zwischen der Europäischen Union und der Union der Komoren vereinbarten Protokoll zur Festlegung der Fangmöglichkeiten und der finanziellen Gegenleistung nach dem partnerschaftlichen Fischereiabkommen zwischen den beiden Vertragsparteien (nachstehend „Protokoll“) festgesetzten Fangmöglichkeiten werden wie folgt auf die Mitgliedstaaten aufgeteilt:\\na)\\n42 Thunfischwadenfänger,\\n-\\nSpanien: 21 Schiffe\\n-\\nFrankreich: 21 Schiffe\\nb)\\n20 Oberflächen-Langleiner:\\n-\\nSpanien: 8 Schiffe\\n-\\nFrankreich: 9 Schiffe\\n-\\nPortugal: 3 Schiffe\\n(2) Die Verordnung (EG) Nr. 1006/2008 gilt unbeschadet des Protokolls und des partnerschaftlichen.\\n(3) Die Frist, innerhalb der die Mitgliedstaaten gemäß Artikel 10 Absatz 1 der Verordnung (EG) Nr. 1006/2008 bestätigen müssen, dass sie die im Rahmen des partnerschaftlichen Fischereiabkommens eingeräumten Fangmöglichkeiten nicht vollständig in Anspruch nehmen, wird auf zehn Arbeitstage ab dem Zeitpunkt der Unterrichtung durch die Kommission, dass die Fangmöglichkeiten nicht ausgeschöpft sind, festgesetzt.\\nArtikel 2\\nDiese Verordnung tritt am Tag nach ihrer Veröffentlichung im Amtsblatt der Europäischen Union in Kraft.\\nSie gilt ab dem 1. Januar 2014.\\nDiese Verordnung ist in allen ihren Teilen verbindlich und gilt unmittelbar in jedem Mitgliedstaat.\\nGeschehen zu Brüssel am 16. Dezember 2013.', 'label_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '__index_level_0__': 4996, 'input_ids': [0, 34237, 36639, 839, 19229, 15, 20214, 16, 9308, 5, 702, 5039, 25105, 51552, 19400, 31273, 4858, 6849, 51681, 1210, 1659, 68, 7601, 66502, 122, 563, 1463, 98635, 1561, 745, 14568, 122, 85329, 19, 32528, 165, 122, 32528, 122, 4440, 38688, 171543, 33, 198154, 2957, 18305, 117257, 122, 563, 1463, 98635, 165, 122, 153136, 19, 41619, 98043, 1561, 745, 4755, 18917, 12512, 136290, 1399, 2055, 20956, 14568, 168, 32207, 109119, 30738, 6594, 6, 18044, 6, 34918, 6, 18044, 64352, 683, 9968, 9956, 215180, 8274, 40713, 20, 700, 243218, 644, 168, 113211, 1659, 68, 23849, 45999, 122, 85329, 19, 32528, 4, 73053, 644, 15387, 6260, 198776, 138, 4, 644, 4855, 56309, 122, 85329, 19, 6, 125117, 4, 23, 1004, 434, 973, 40364, 1561, 113503, 42, 97764, 12, 798, 2022, 1892, 18146, 3295, 1256, 122, 21109, 68, 195390, 15, 25176, 16, 9308, 5, 423, 15748, 72793, 798, 1659, 168, 116249, 224, 4755, 18917, 12512, 136290, 1399, 2055, 20956, 7, 14568, 122, 85329, 19, 154501, 165, 122, 32528, 122, 4440, 38688, 15, 464, 88439, 555, 137, 27649, 18917, 100300, 136290, 1399, 2055, 20956, 155, 16, 142, 61531, 5, 1737, 622, 85329, 32528, 1256, 491, 122, 32528, 122, 4440, 38688, 599, 58645, 198154, 2388, 4755, 18917, 12512, 136290, 1399, 2055, 20956, 15, 12177, 114180, 71, 137, 86, 21049, 198154, 155, 16, 20609, 16139, 18, 4, 491, 745, 168, 136290, 1399, 213093, 33, 122, 85329, 19, 32528, 563, 1463, 98635, 23, 168, 77589, 7372, 2206, 243187, 19, 37194, 129906, 18, 1331, 5, 11022, 116249, 122, 210485, 33, 4180, 444, 1892, 19838, 1210, 599, 58645, 198154, 121, 19379, 2529, 5, 2788, 2022, 6849, 51681, 1210, 1256, 122, 21109, 168, 873, 54719, 1210, 64, 172228, 64, 20214, 1737, 1659, 68, 21912, 106566, 165, 1248, 187992, 13, 77508, 224, 16099, 198154, 7, 142, 61531, 5, 3971, 1184, 443, 157452, 4, 68, 563, 1463, 98635, 643, 68, 77508, 7, 107536, 224, 16099, 198154, 7, 644, 68, 67386, 49400, 33, 104488, 69431, 5, 5211, 160365, 18, 833, 44695, 4, 1421, 68, 122, 85329, 19, 32528, 566, 36070, 224, 16099, 198154, 7, 37194, 129906, 510, 563, 1463, 98635, 749, 112340, 20609, 3232, 70740, 2480, 1331, 4, 221, 6, 184838, 126, 68, 6, 125117, 126854, 122, 195390, 15, 25176, 16, 9308, 5, 805, 910, 57093, 224, 131182, 7, 2788, 68, 128806, 19, 67386, 49400, 33, 1791, 36883, 5, 67322, 18, 48577, 2235, 4858, 21109, 7594, 186097, 555, 191172, 6731, 94670, 599, 4, 221, 25615, 14792, 737, 234119, 4, 1421, 68, 130275, 13, 224, 128806, 19, 67386, 49400, 7, 9047, 563, 1463, 98635, 23, 745, 128806, 19, 141195, 749, 59848, 23, 93596, 31795, 5, 13829, 191172, 12961, 198333, 1331, 5, 6791, 46000, 68, 136290, 1399, 213093, 13, 122, 85329, 19, 32528, 9047, 563, 1463, 226845, 33, 749, 3993, 138558, 13286, 4, 36805, 381, 11157, 198154, 87827, 1248, 187992, 13, 77508, 3471, 68, 109119, 30738, 6594, 1563, 745, 615, 47171, 1049, 1248, 5, 4855, 50830, 112, 195390, 12961, 46163, 13154, 1563, 122, 1248, 187992, 33, 77508, 224, 16099, 198154, 7, 69227, 20, 6, 43329, 44944, 39303, 73661, 34237, 36639, 839, 19229, 28695, 46705, 29294, 12, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(test_datasets['de'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86566ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b84c1",
   "metadata": {},
   "source": [
    "# with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d51800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 54994\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4423a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88eed304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"label_vector\"])\n",
    "print(type(train_dataset[0][\"label_vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f23a6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54994/54994 [00:39<00:00, 1377.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for ex in tqdm(train_dataset):\n",
    "    lv = ex[\"label_vector\"]\n",
    "    if not isinstance(lv, list) or not all(isinstance(i, int) for i in lv):\n",
    "        print(\"Invalid label_vector:\", lv)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f5e3550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:17<00:00, 3112.98 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 3596.54 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 2886.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(example):\n",
    "    example[\"labels\"] = example[\"label_vector\"]\n",
    "    return example\n",
    "train_dataset = train_dataset.map(prepare_dataset)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "123d285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Verify column names before setting format\n",
    "print(train_dataset.column_names)  # This should show the column names if the dataset is valid\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25c376ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"label_vector\"])\n",
    "print(type(train_dataset[0][\"label_vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f49afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04e51b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label_vector', '__index_level_0__', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6724975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [celex_id, publication_date, text, eurovoc_concepts, split, doc_lengths, max_doc_length, level_3_labels, label_vector]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "empty_labels = train_df[train_df['label_vector'].apply(lambda x: len(x) == 0)]\n",
    "print(empty_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78e90cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: <class 'torch.Tensor'> - shape: torch.Size([4, 512])\n",
      "attention_mask: <class 'torch.Tensor'> - shape: torch.Size([4, 512])\n",
      "labels: <class 'torch.Tensor'> - shape: torch.Size([4, 500])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: {type(v)} - shape: {v.shape if hasattr(v, 'shape') else 'N/A'}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a445e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], dtype={\"labels\": torch.float32})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df3101aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'column_names'"
     ]
    }
   ],
   "source": [
    "print(test_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f16eacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_datasets_loader = DataLoader(test_datasets, batch_size=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_datasets_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m - shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mshape\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mN/A\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "test_datasets_loader = DataLoader(test_datasets, batch_size=4)\n",
    "\n",
    "for batch in test_datasets_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: {type(v)} - shape: {v.shape if hasattr(v, 'shape') else 'N/A'}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c71d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:20<00:00, 2667.03 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 3516.65 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 3459.28 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:01<00:00, 2560.81 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:02<00:00, 2080.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# last modif\n",
    "def prepare_dataset(example):\n",
    "    example[\"labels\"] = example[\"label_vector\"]\n",
    "    return example\n",
    "train_dataset = train_dataset.map(prepare_dataset)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(prepare_dataset)\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    dtype={\"input_ids\": torch.int64, \"attention_mask\": torch.int64, \"labels\": torch.float32}\n",
    ")\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], dtype={\"labels\": torch.float32, \"attention_mask\": torch.int64, \"labels\": torch.float32})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4ce7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of labels\n",
    "num_labels = len(mlb.classes_)\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(mlb.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(mlb.classes_)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66779692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4251c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('we love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a3dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm-roberta-eurovoc\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_dir=\"./logs\",                    # Log directory\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0769ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39baca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions  # ← grab predictions from the object\n",
    "    labels = pred.label_ids    # ← grab true labels\n",
    "\n",
    "    probs = torch.sigmoid(torch.from_numpy(logits))  # safely convert to tensor\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\"),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b180431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78158/1019803887.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tensor(): argument 'dtype' must be torch.dtype, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     model=model,\n\u001b[32m      4\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/accelerate/data_loader.py:566\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2781\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2780\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2781\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2782\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/formatting.py:666\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    665\u001b[39m     pa_table_to_format = pa_table.drop(col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table.column_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m format_columns)\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     formatted_output = \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table_to_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/formatting.py:415\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:119\u001b[39m, in \u001b[36mTorchFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    117\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.numpy_arrow_extractor().extract_batch(pa_table)\n\u001b[32m    118\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecursive_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    121\u001b[39m     batch[column_name] = \u001b[38;5;28mself\u001b[39m._consolidate(batch[column_name])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:102\u001b[39m, in \u001b[36mTorchFormatter.recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecursive_tensorize\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_struct: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recursive_tensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_list\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/utils/py_utils.py:514\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    511\u001b[39m         batch_size = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) // num_proc + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) % num_proc > \u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m    512\u001b[39m     iterable = \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[32m    513\u001b[39m mapped = [\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\n\u001b[32m    516\u001b[39m ]\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    518\u001b[39m     mapped = [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/utils/py_utils.py:375\u001b[39m, in \u001b[36m_single_map_nested\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    373\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m function([data_struct])[\u001b[32m0\u001b[39m]\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    377\u001b[39m     batched\n\u001b[32m    378\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    379\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[32m    380\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[32m    381\u001b[39m ):\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:99\u001b[39m, in \u001b[36mTorchFormatter._recursive_tensorize\u001b[39m\u001b[34m(self, data_struct)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consolidate([\u001b[38;5;28mself\u001b[39m.recursive_tensorize(substruct) \u001b[38;5;28;01mfor\u001b[39;00m substruct \u001b[38;5;129;01min\u001b[39;00m data_struct])\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tensorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85\u001b[39m, in \u001b[36mTorchFormatter._tensorize\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, VideoReader):\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value  \u001b[38;5;66;03m# TODO(QL): set output to torch tensors ?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_tensor_kwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: tensor(): argument 'dtype' must be torch.dtype, not dict"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_datasets[\"fr\"],  # Or \"de\", \"es\" — you can loop through them too\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c75a0a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "train_dataset['labels'][0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cceebea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 5 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.dtype)\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 5 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor(train_dataset['labels']).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e908a0",
   "metadata": {},
   "source": [
    "# with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f8cc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_BACKEND\"] = \"pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd49a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\tf_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guibe\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\tf_env\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# Get the number of labels\n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(mlb.classes_)},\n",
    "    label2id={label: i for i, label in enumerate(mlb.classes_)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71e91f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:42<00:00, 1288.60 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:04<00:00, 1242.97 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:03<00:00, 1313.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(example):\n",
    "    example[\"labels\"] = example[\"label_vector\"]\n",
    "    return example\n",
    "\n",
    "# Apply the transformation for training and test datasets\n",
    "train_dataset = train_dataset.map(prepare_dataset)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(prepare_dataset)\n",
    "\n",
    "# Set format for TensorFlow\n",
    "train_dataset.set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang].set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdfab39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\"),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0de79e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Adjust as per your specific task\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     problem_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\tf_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\tf_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=2,  # Adjust as per your specific task\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff1ea901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained TensorFlow model and tokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f46403a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the training arguments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./test_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Log directory\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensorboard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Use tensorboard for logging\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",      \n",
    "    evaluation_strategy=\"epoch\",             \n",
    "    save_strategy=\"epoch\",                   \n",
    "    learning_rate=2e-5,                      \n",
    "    per_device_train_batch_size=8,           \n",
    "    per_device_eval_batch_size=8,            \n",
    "    num_train_epochs=3,                      \n",
    "    weight_decay=0.01,                       \n",
    "    save_total_limit=1,                      \n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",                    # Log directory\n",
    "    report_to=\"tensorboard\"                  # Use tensorboard for logging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce0f3a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define your training arguments (adjust hyperparameters as needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./xlm-roberta-eurovoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicro_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Define your training arguments (adjust hyperparameters as needed)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm-roberta-eurovoc\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_datasets[\"fr\"],  # Or \"de\", \"es\" — you can loop through them too\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, dataset in test_datasets.items():\n",
    "    results = trainer.evaluate(dataset)\n",
    "    print(f\"Language: {lang}\")\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb35db",
   "metadata": {},
   "source": [
    "## Test with article code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65864aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guibe\\AppData\\Local\\Temp\\ipykernel_14516\\3094304611.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['label_vector'] = list(mlb.transform(train_df['level_3_labels']))\n",
      "C:\\Users\\guibe\\AppData\\Local\\Temp\\ipykernel_14516\\3094304611.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['label_vector'] = list(mlb.transform(test_df['level_3_labels']))\n"
     ]
    }
   ],
   "source": [
    "# label encoding\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_df['level_3_labels'])\n",
    "\n",
    "train_df['label_vector'] = list(mlb.transform(train_df['level_3_labels']))\n",
    "test_df['label_vector'] = list(mlb.transform(test_df['level_3_labels']))\n",
    "\n",
    "label_index = {label: idx for idx, label in enumerate(mlb.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "streamed_dataset = load_dataset(\"path/to/your/data.csv\", split=\"train\", streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04637ab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 826277888 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset \u001b[38;5;28;01mas\u001b[39;00m HFDataset\n\u001b[1;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m HFDataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_vector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelex_id\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m HFDataset\u001b[38;5;241m.\u001b[39mfrom_pandas(test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_vector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelex_id\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\datasets\\arrow_dataset.py:841\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    839\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n\u001b[0;32m    840\u001b[0m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m--> 841\u001b[0m table \u001b[38;5;241m=\u001b[39m InMemoryTable\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    842\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m    843\u001b[0m     preserve_index\u001b[38;5;241m=\u001b[39mpreserve_index,\n\u001b[0;32m    844\u001b[0m )\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcast(features\u001b[38;5;241m.\u001b[39marrow_schema)\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\datasets\\table.py:720\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;124;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\pandas_compat.py:638\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m--> 638\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m maybe_fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    640\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\pandas_compat.py:606\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    603\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 606\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(col, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtype_, from_pandas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, safe\u001b[38;5;241m=\u001b[39msafe)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    608\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    609\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    610\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    611\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\array.pxi:360\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\array.pxi:87\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\guibe\\anaconda3\\envs\\vscode\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 826277888 failed"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "train_dataset = HFDataset.from_pandas(train_df[['text', 'label_vector', 'celex_id']])\n",
    "test_dataset = HFDataset.from_pandas(test_df[['text', 'label_vector', 'celex_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6772bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_df = train_df.sample(1000).copy()\n",
    "train_dataset = HFDataset.from_pandas(small_train_df[['text', 'label_vector', 'celex_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4a108b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_df = test_df.sample(500).copy()\n",
    "test_dataset = HFDataset.from_pandas(small_test_df[['text', 'label_vector', 'celex_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "704464cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Classifier\n",
      "File \u001b[1;32mc:\\Users\\guibe\\OneDrive\\Documents\\ENSAE\\3A\\S2\\NLP\\projet\\NLP-Legal-document-classification\\experiments\\model.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Adapters code inspired by\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Hosein Mohebbi's project: \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# https://github.com/hmohebbi/TF-Adapter-BERT\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from experiments.model import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef548aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(bert_model_path='xlm-roberta-base', num_labels=len(label_index))\n",
    "model.adapt_model(use_adapters=True, num_frozen_layers=None)  # Or skip this for baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570bc0b",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = SampleGenerator(train_dataset, label_index, 'xlm-roberta-base', lang='en', multilingual_train=False)\n",
    "test_gen = SampleGenerator(test_dataset, label_index, 'xlm-roberta-base', lang=['fr', 'de', 'es', 'it'], multilingual_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e427f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13819abe",
   "metadata": {},
   "source": [
    "Suggested Steps to Fix the Issue:\n",
    "Downgrade Python to a Compatible Version (3.7–3.10): To resolve the issue, I recommend downgrading Python to a version that TensorFlow supports, ideally Python 3.10. Here's how to do it:\n",
    "\n",
    "Step 1: Install Python 3.10\n",
    "Download Python 3.10: Go to the Python 3.10 download page and download the installer for your operating system.\n",
    "\n",
    "Install Python 3.10: During installation, make sure to check the box that says \"Add Python to PATH\" to make it accessible from the command line.\n",
    "\n",
    "Step 2: Create a Virtual Environment with Python 3.10\n",
    "After installing Python 3.10, create a new virtual environment:\n",
    "\n",
    "Windows:\n",
    "\n",
    "bash\n",
    "Copier\n",
    "Modifier\n",
    "python3.10 -m venv tf_env\n",
    ".\\tf_env\\Scripts\\activate\n",
    "macOS/Linux:\n",
    "\n",
    "bash\n",
    "Copier\n",
    "Modifier\n",
    "python3.10 -m venv tf_env\n",
    "source tf_env/bin/activate\n",
    "Step 3: Install TensorFlow in the New Virtual Environment\n",
    "After creating and activating your new environment, install TensorFlow:\n",
    "\n",
    "bash\n",
    "Copier\n",
    "Modifier\n",
    "pip install tensorflow\n",
    "Verify TensorFlow Installation: Once TensorFlow is installed, verify that the installation is successful by running:\n",
    "\n",
    "python\n",
    "Copier\n",
    "Modifier\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "This should print the TensorFlow version without errors.\n",
    "\n",
    "Alternative: Using Docker for Isolation (Optional)\n",
    "If you prefer not to downgrade Python globally or create a new Python installation, you can use Docker to run a TensorFlow-compatible environment in an isolated container. Docker allows you to run a specific version of Python and TensorFlow without affecting your system-wide Python installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f06ab9",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "46125765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60676/598392232.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df[\"text\"].apply(lambda x: isinstance(x, dict) and x.get(\"en\"))\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_df = df_reduced[df_reduced['split']=='train']\n",
    "train_df['text'] = train_df[\"text\"].apply(lambda x: isinstance(x, dict) and x.get(\"en\"))\n",
    "\n",
    "# test \n",
    "test_df = df_reduced[df_reduced['split']=='test']\n",
    "test_langs = [\"fr\", \"de\", \"pl\",\"fi\"] \n",
    "test_dfs = []\n",
    "\n",
    "for lang in test_langs:\n",
    "    # Filter rows where the language exists in the text dictionary\n",
    "    df_lang = test_df[test_df[\"text\"].apply(lambda x: isinstance(x, dict) and lang in x)]\n",
    "    \n",
    "    # Now extract the respective language text, and add the 'lang' column\n",
    "    df_lang[\"text\"] = df_lang[\"text\"].apply(lambda x: x[lang])  # Extract the language text\n",
    "    df_lang[\"lang\"] = lang  # Add a new column for language\n",
    "    \n",
    "    # Append to test_dfs\n",
    "    test_dfs.append(df_lang)\n",
    "\n",
    "# Combine the list of DataFrames into one (exploded test set)\n",
    "final_test_df = pd.concat(test_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8803699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60676/3239486456.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['label_vector'] = mlb.fit_transform(train_df['level_3_labels']).tolist()\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['41', '6138', '7815', '7817', '7855', '7861', '7868', '7879', '8301', '962'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: label_vector, dtype: object\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: label_vector, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Initialize the MultiLabelBinarizer for the 'level_3_labels'\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Encode the 'level_3_labels' column\n",
    "train_df['label_vector'] = mlb.fit_transform(train_df['level_3_labels']).tolist()\n",
    "\n",
    "# You can optionally do this for the test set as well if you have labels for the test data\n",
    "final_test_df['label_vector'] = mlb.transform(final_test_df['level_3_labels']).tolist()\n",
    "\n",
    "# Check the resulting label vectors (optional)\n",
    "print(train_df['label_vector'].head())\n",
    "print(final_test_df['label_vector'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e93163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [00:39<00:00, 1390.91 examples/s]\n",
      "Map: 100%|██████████| 19984/19984 [00:26<00:00, 756.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'celex_id': '32006D0213', 'publication_date': '2006-03-06', 'text': 'COMMISSION DECISION\\nof 6 March 2006\\nestablishing the classes of reaction-to-fire performance for certain construction products as regards wood flooring and solid wood panelling and cladding\\n(notified under document number C(2006) 655)\\n(Text with EEA relevance)\\n(2006/213/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Directive 89/106/EEC of 21 December 1988, on the approximation of laws, regulations and administrative provisions of the Member States relating to construction products (1), and in particular Article 20(2) thereof,\\nWhereas:\\n(1)\\nDirective 89/106/EEC envisages that in order to take account of different levels of protection for construction works at national, regional or local level, it may be necessary to establish in the interpretative documents classes corresponding to the performance of products in respect of each essential requirement. Those documents have been published as the ‘Communication of the Commission with regard to the interpretative documents of Directive 89/106/EEC’ (2).\\n(2)\\nWith respect to the essential requirement of safety in the event of fire, interpretative document No 2 lists a number of interrelated measures which together define the fire safety strategy to be variously developed in the Member States.\\n(3)\\nInterpretative document No 2 identifies one of those measures as the limitation of the generation and spread of fire and smoke within a given area by limiting the potential of construction products to contribute to the full development of a fire.\\n(4)\\nThe level of that limitation may be expressed only in terms of the different levels of reaction-to-fire performance of the products in their end-use application;\\n(5)\\nBy way of harmonised solution, a system of classes was adopted in Commission Decision 2000/147/EC of 8 February 2000 implementing Council Directive 89/106/EEC as regards the classification of the reaction-to-fire performance of construction products (3).\\n(6)\\nIn the case of wood flooring and solid wood panelling and cladding it is necessary to use the classification established in Decision 2000/147/EC.\\n(7)\\nThe reaction-to-fire performance of many construction products and/or materials, within the classification provided for in Decision 2000/147/EC, is well established and sufficiently well known to fire regulators in Member States that they do not require testing for this particular performance characteristic.\\n(8)\\nThe measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Construction,\\nHAS ADOPTED THIS DECISION:\\nArticle 1\\nThe construction products and/or materials which satisfy all the requirements of the performance characteristic ‘reaction to fire’ without need for further testing are set out in the Annex.\\nArticle 2\\nThe specific classes to be applied to different construction products and/or materials, within the reaction-to-fire classification adopted in Decision 2000/147/EC, are set out in the Annex to this Decision.\\nArticle 3\\nProducts shall be considered in relation to their end-use application, where relevant.\\nArticle 4\\nThis Decision is addressed to the Member States.\\nDone at Brussels, 6 March 2006.', 'eurovoc_concepts': {'all_levels': ['1706', '1826', '2754', '3690', '4036', '5234', '5309'], 'level_1': ['100160', '100155', '100158', '100147', '100149'], 'level_2': ['100273', '100216', '100261', '100274', '100195', '100242'], 'level_3': ['1386', '2825', '138', '2475', '3879', '3641']}, 'split': 'train', 'doc_lengths': {'de': 3302, 'en': 3233, 'fi': 3073, 'fr': 3642, 'pl': 3326}, 'max_doc_length': 3642, 'level_3_labels': ['1386', '2825', '138', '2475', '3879', '3641'], 'label_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '__index_level_0__': 0, 'input_ids': [101, 3222, 3247, 1997, 1020, 2233, 2294, 7411, 1996, 4280, 1997, 4668, 1011, 2000, 1011, 2543, 2836, 2005, 3056, 2810, 3688, 2004, 12362, 3536, 2723, 2075, 1998, 5024, 3536, 5997, 2989, 1998, 13681, 4667, 1006, 19488, 2104, 6254, 2193, 1039, 1006, 2294, 1007, 3515, 2629, 1007, 1006, 3793, 2007, 25212, 2050, 21923, 1007, 1006, 2294, 1013, 19883, 1013, 14925, 1007, 1996, 3222, 1997, 1996, 2647, 4279, 1010, 2383, 7634, 2000, 1996, 5036, 7411, 1996, 2647, 2451, 1010, 2383, 7634, 2000, 16449, 6486, 1013, 10114, 1013, 25212, 2278, 1997, 2538, 2285, 2997, 1010, 2006, 1996, 20167, 1997, 4277, 1010, 7040, 1998, 3831, 8910, 1997, 1996, 2266, 2163, 8800, 2000, 2810, 3688, 1006, 1015, 1007, 1010, 1998, 1999, 3327, 3720, 2322, 1006, 1016, 1007, 21739, 1010, 6168, 1024, 1006, 1015, 1007, 16449, 6486, 1013, 10114, 1013, 25212, 2278, 4372, 11365, 13923, 2008, 1999, 2344, 2000, 2202, 4070, 1997, 2367, 3798, 1997, 3860, 2005, 2810, 2573, 2012, 2120, 1010, 3164, 2030, 2334, 2504, 1010, 2009, 2089, 2022, 4072, 2000, 5323, 1999, 1996, 17841, 8082, 5491, 4280, 7978, 2000, 1996, 2836, 1997, 3688, 1999, 4847, 1997, 2169, 6827, 9095, 1012, 2216, 5491, 2031, 2042, 2405, 2004, 1996, 1520, 4807, 1997, 1996, 3222, 2007, 7634, 2000, 1996, 17841, 8082, 5491, 1997, 16449, 6486, 1013, 10114, 1013, 25212, 2278, 1521, 1006, 1016, 1007, 1012, 1006, 1016, 1007, 2007, 4847, 2000, 1996, 6827, 9095, 1997, 3808, 1999, 1996, 2724, 1997, 2543, 1010, 17841, 8082, 6254, 2053, 1016, 7201, 1037, 2193, 1997, 6970, 16570, 4383, 5761, 2029, 2362, 9375, 1996, 2543, 3808, 5656, 2000, 2022, 17611, 2764, 1999, 1996, 2266, 2163, 1012, 1006, 1017, 1007, 17841, 8082, 6254, 2053, 1016, 14847, 2028, 1997, 2216, 5761, 2004, 1996, 22718, 1997, 1996, 4245, 1998, 3659, 1997, 2543, 1998, 5610, 2306, 1037, 2445, 2181, 2011, 14879, 1996, 4022, 1997, 2810, 3688, 2000, 9002, 2000, 1996, 2440, 2458, 1997, 1037, 2543, 1012, 1006, 1018, 1007, 1996, 2504, 1997, 2008, 22718, 2089, 2022, 5228, 2069, 1999, 3408, 1997, 1996, 2367, 3798, 1997, 4668, 1011, 2000, 1011, 2543, 2836, 1997, 1996, 3688, 1999, 2037, 2203, 1011, 2224, 4646, 1025, 1006, 1019, 1007, 2011, 2126, 1997, 25546, 5084, 5576, 1010, 1037, 2291, 1997, 4280, 2001, 4233, 1999, 3222, 3247, 2456, 1013, 16471, 1013, 14925, 1997, 1022, 2337, 2456, 14972, 2473, 16449, 6486, 1013, 10114, 1013, 25212, 2278, 2004, 12362, 1996, 5579, 1997, 1996, 4668, 1011, 2000, 1011, 2543, 2836, 1997, 2810, 3688, 1006, 1017, 1007, 1012, 1006, 1020, 1007, 1999, 1996, 2553, 1997, 3536, 2723, 2075, 1998, 5024, 3536, 5997, 2989, 1998, 13681, 4667, 2009, 2003, 4072, 2000, 2224, 1996, 5579, 2511, 1999, 3247, 2456, 1013, 16471, 1013, 14925, 1012, 1006, 1021, 1007, 1996, 4668, 1011, 2000, 1011, 2543, 2836, 1997, 2116, 2810, 3688, 1998, 1013, 2030, 4475, 1010, 2306, 1996, 5579, 3024, 2005, 1999, 3247, 2456, 1013, 16471, 1013, 14925, 1010, 2003, 2092, 2511, 1998, 12949, 2092, 2124, 2000, 2543, 25644, 1999, 2266, 2163, 2008, 2027, 2079, 2025, 5478, 5604, 2005, 2023, 3327, 2836, 8281, 1012, 1006, 1022, 1007, 1996, 5761, 3024, 2005, 1999, 2023, 3247, 2024, 1999, 10388, 2007, 1996, 5448, 1997, 1996, 3061, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'celex_id': '32013R1390', 'publication_date': '2013-12-16', 'text': \"RÈGLEMENT (UE) No 1390/2013 DU CONSEIL\\ndu 16 décembre 2013\\nrelatif à la répartition des possibilités de pêche au titre du protocole entre l'Union européenne et l'Union des Comores fixant les possibilités de pêche et la contrepartie financière prévues par l'accord de partenariat dans le secteur de la pêche en vigueur entre les deux parties\\nLE CONSEIL DE L'UNION EUROPÉENNE,\\nvu le traité sur le fonctionnement de l'Union européenne, et notamment son article 43, paragraphe 3,\\nvu la proposition de la Commission européenne,\\nconsidérant ce qui suit:\\n(1)\\nLe 5 octobre 2006, le Conseil a approuvé l'accord de partenariat dans le secteur de la pêche entre la Communauté européenne et l'Union des Comores (ci-après dénommé «accord de partenariat») en adoptant le règlement (CE) no 1563/2006 (1).\\n(2)\\nL'Union européenne a négocié avec l'Union des Comores un nouveau protocole à l'accord de partenariat accordant aux navires de l'Union européenne des possibilités de pêche dans les eaux comoriennes. À l'issue des négociations, un nouveau protocole a été paraphé le 5 juillet 2013.\\n(3)\\nLe 16 décembre 2013, le Conseil a adopté la décision 2013/786/UE (2) relative à la signature et à l'application provisoire du nouveau protocole.\\n(4)\\nIl convient de répartir des possibilités de pêche entre les États membres pour la période d'application du nouveau protocole.\\n(5)\\nConformément au règlement (CE) no 1006/2008 du Conseil (3), s'il apparaît que les possibilités de pêche allouées à l'Union européenne en vertu du nouveau protocole ne sont pas pleinement utilisées, la Commission en informe les États membres concernés. L'absence de réponse dans un délai à fixer par le Conseil est à considérer comme une confirmation que les navires de l'État membre concerné n'utilisent pas pleinement leurs possibilités de pêche pendant la période considérée. Il convient de fixer ledit délai.\\n(6)\\nAfin d'assurer la poursuite des activités de pêche des navires de l'Union européenne, le nouveau protocole prévoit la possibilité de son application à titre provisoire par chacune des parties à compter du 1er janvier 2014. Il convient donc que le présent règlement s'applique dès l'application provisoire du nouveau protocole,\\nA ADOPTÉ LE PRÉSENT RÈGLEMENT:\\nArticle premier\\n1. Les possibilités de pêche fixées par le protocole entre l'Union européenne et l'Union des Comores fixant les possibilités de pêche et la contrepartie financière prévues par l'accord de partenariat dans le secteur de la pêche en vigueur entre les deux parties (ci-après dénommé «protocole») sont réparties comme suit entre les États membres:\\na)\\n42 thoniers senneurs:\\n-\\nEspagne: 21 navires\\n-\\nFrance: 21 navires\\nb)\\n20 palangriers de surface:\\n-\\nEspagne: 8 navires\\n-\\nFrance: 9 navires\\n-\\nPortugal: 3 navires\\n2. Le règlement (CE) no 1006/2008 s'applique sans préjudice du protocole et de l'accord de partenariat.\\n3. Le délai dans lequel les États membres sont tenus de confirmer qu'ils n'utilisent pas pleinement les possibilités de pêche accordées au titre de l'accord de partenariat, tel que visé a l'article 10, paragraphe 1, du règlement (CE) no 1006/2008, est fixé à dix jours ouvrables à partir de la date à laquelle la Commission les informe que les possibilités de pêche ne sont pas pleinement utilisées.\\nArticle 2\\nLe présent règlement entre en vigueur le jour suivant celui de sa publication au Journal officiel de l'Union européenne.\\nIl est applicable à partir du 1er janvier 2014.\\nLe présent règlement est obligatoire dans tous ses éléments et directement applicable dans tout État membre.\\nFait à Bruxelles, le 16 décembre 2013.\", 'eurovoc_concepts': {'all_levels': ['1085', '1474', '2329', '2556', '2563', '2850', '33', '4790', '544', '863'], 'level_1': ['100161', '100144', '100143', '100156'], 'level_2': ['100280', '100277', '100283', '100252', '100282', '100170', '100176'], 'level_3': ['3461', '5424', '5283', '122', '2106', '4590', '913', '4040', '912', '2200', '2476']}, 'split': 'test', 'doc_lengths': {'de': 3728, 'en': 3410, 'fi': 3364, 'fr': 3582, 'pl': 3426}, 'max_doc_length': 3728, 'level_3_labels': ['3461', '5424', '5283', '122', '2106', '4590', '913', '4040', '912', '2200', '2476'], 'lang': 'fr', 'label_vector': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 19723, 16930, 4765, 1006, 1057, 2063, 1007, 2053, 16621, 2692, 1013, 2286, 4241, 9530, 20240, 2140, 4241, 2385, 11703, 6633, 13578, 2286, 2128, 20051, 10128, 1037, 2474, 16360, 8445, 22753, 4078, 13433, 18719, 14454, 7616, 2139, 21877, 5403, 8740, 14841, 7913, 4241, 8778, 2063, 4372, 7913, 1048, 1005, 2586, 2885, 24336, 3802, 1048, 1005, 2586, 4078, 18609, 6072, 8081, 4630, 4649, 13433, 18719, 14454, 7616, 2139, 21877, 5403, 3802, 2474, 9530, 7913, 19362, 9515, 10346, 2319, 19562, 2063, 3653, 19722, 2229, 11968, 1048, 1005, 15802, 2139, 2112, 8189, 4360, 2102, 18033, 3393, 17831, 11236, 2139, 2474, 21877, 5403, 4372, 6819, 9077, 3126, 4372, 7913, 4649, 24756, 4243, 3393, 9530, 20240, 2140, 2139, 1048, 1005, 2586, 2885, 24336, 1010, 24728, 3393, 18275, 2063, 7505, 3393, 1042, 2239, 7542, 25832, 4765, 2139, 1048, 1005, 2586, 2885, 24336, 1010, 3802, 2025, 3286, 3672, 2365, 3720, 4724, 1010, 20423, 2063, 1017, 1010, 24728, 2474, 14848, 2139, 2474, 3222, 2885, 24336, 1010, 5136, 4630, 8292, 21864, 4848, 1024, 1006, 1015, 1007, 3393, 1019, 13323, 16429, 2890, 2294, 1010, 3393, 9530, 20240, 2140, 1037, 10439, 22494, 3726, 1048, 1005, 15802, 2139, 2112, 8189, 4360, 2102, 18033, 3393, 17831, 11236, 2139, 2474, 21877, 5403, 4372, 7913, 2474, 4012, 23041, 4887, 2618, 2885, 24336, 3802, 1048, 1005, 2586, 4078, 18609, 6072, 1006, 25022, 1011, 19804, 2229, 7939, 5358, 4168, 1077, 15802, 2139, 2112, 8189, 4360, 2102, 1090, 1007, 4372, 11092, 4630, 3393, 19723, 16930, 4765, 1006, 8292, 1007, 2053, 16734, 2509, 1013, 2294, 1006, 1015, 1007, 1012, 1006, 1016, 1007, 1048, 1005, 2586, 2885, 24336, 1037, 11265, 3995, 23402, 13642, 2278, 1048, 1005, 2586, 4078, 18609, 6072, 4895, 25272, 8778, 2063, 1037, 1048, 1005, 15802, 2139, 2112, 8189, 4360, 2102, 15802, 4630, 19554, 6583, 21663, 2229, 2139, 1048, 1005, 2586, 2885, 24336, 4078, 13433, 18719, 14454, 7616, 2139, 21877, 5403, 18033, 4649, 19413, 5602, 18609, 23144, 5267, 1012, 1037, 1048, 1005, 3277, 4078, 11265, 3995, 23247, 2015, 1010, 4895, 25272, 8778, 2063, 1037, 3802, 2063, 11498, 8458, 2063, 3393, 1019, 18414, 10484, 2102, 2286, 1012, 1006, 1017, 1007, 3393, 2385, 11703, 6633, 13578, 2286, 1010, 3393, 9530, 20240, 2140, 1037, 11092, 2063, 2474, 3247, 2286, 1013, 6275, 2575, 1013, 1057, 2063, 1006, 1016, 1007, 5816, 1037, 2474, 8085, 3802, 1037, 1048, 1005, 4646, 4013, 11365, 26250, 4241, 25272, 8778, 2063, 1012, 1006, 1018, 1007, 6335, 9530, 13469, 3372, 2139, 16360, 8445, 4313, 4078, 13433, 18719, 14454, 7616, 2139, 21877, 5403, 4372, 7913, 4649, 17997, 2015, 2033, 19908, 2015, 10364, 2474, 2558, 2063, 1040, 1005, 4646, 4241, 25272, 8778, 2063, 1012, 1006, 1019, 1007, 23758, 13665, 8740, 19723, 16930, 4765, 1006, 8292, 1007, 2053, 2531, 2575, 1013, 2263, 4241, 9530, 20240, 2140, 1006, 1017, 1007, 1010, 1055, 1005, 6335, 10439, 5400, 4183, 10861, 4649, 13433, 18719, 14454, 7616, 2139, 21877, 5403, 2035, 27872, 2229, 1037, 1048, 1005, 2586, 2885, 24336, 4372, 2310, 5339, 2226, 4241, 25272, 8778, 2063, 11265, 2365, 2102, 14674, 20228, 12377, 13665, 21183, 24411, 10285, 1010, 2474, 3222, 4372, 12367, 2063, 4649, 17997, 2015, 2033, 19908, 2015, 5142, 2229, 1012, 1048, 1005, 6438, 2139, 16360, 5644, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (for example, using BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Convert the train and test DataFrames to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(final_test_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the tokenized datasets\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "baf1612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50', 51: 'LABEL_51', 52: 'LABEL_52', 53: 'LABEL_53', 54: 'LABEL_54', 55: 'LABEL_55', 56: 'LABEL_56', 57: 'LABEL_57', 58: 'LABEL_58', 59: 'LABEL_59', 60: 'LABEL_60', 61: 'LABEL_61', 62: 'LABEL_62', 63: 'LABEL_63', 64: 'LABEL_64', 65: 'LABEL_65', 66: 'LABEL_66', 67: 'LABEL_67', 68: 'LABEL_68', 69: 'LABEL_69', 70: 'LABEL_70', 71: 'LABEL_71', 72: 'LABEL_72', 73: 'LABEL_73', 74: 'LABEL_74', 75: 'LABEL_75', 76: 'LABEL_76', 77: 'LABEL_77', 78: 'LABEL_78', 79: 'LABEL_79', 80: 'LABEL_80', 81: 'LABEL_81', 82: 'LABEL_82', 83: 'LABEL_83', 84: 'LABEL_84', 85: 'LABEL_85', 86: 'LABEL_86', 87: 'LABEL_87', 88: 'LABEL_88', 89: 'LABEL_89', 90: 'LABEL_90', 91: 'LABEL_91', 92: 'LABEL_92', 93: 'LABEL_93', 94: 'LABEL_94', 95: 'LABEL_95', 96: 'LABEL_96', 97: 'LABEL_97', 98: 'LABEL_98', 99: 'LABEL_99', 100: 'LABEL_100', 101: 'LABEL_101', 102: 'LABEL_102', 103: 'LABEL_103', 104: 'LABEL_104', 105: 'LABEL_105', 106: 'LABEL_106', 107: 'LABEL_107', 108: 'LABEL_108', 109: 'LABEL_109', 110: 'LABEL_110', 111: 'LABEL_111', 112: 'LABEL_112', 113: 'LABEL_113', 114: 'LABEL_114', 115: 'LABEL_115', 116: 'LABEL_116', 117: 'LABEL_117', 118: 'LABEL_118', 119: 'LABEL_119', 120: 'LABEL_120', 121: 'LABEL_121', 122: 'LABEL_122', 123: 'LABEL_123', 124: 'LABEL_124', 125: 'LABEL_125', 126: 'LABEL_126', 127: 'LABEL_127', 128: 'LABEL_128', 129: 'LABEL_129', 130: 'LABEL_130', 131: 'LABEL_131', 132: 'LABEL_132', 133: 'LABEL_133', 134: 'LABEL_134', 135: 'LABEL_135', 136: 'LABEL_136', 137: 'LABEL_137', 138: 'LABEL_138', 139: 'LABEL_139', 140: 'LABEL_140', 141: 'LABEL_141', 142: 'LABEL_142', 143: 'LABEL_143', 144: 'LABEL_144', 145: 'LABEL_145', 146: 'LABEL_146', 147: 'LABEL_147', 148: 'LABEL_148', 149: 'LABEL_149', 150: 'LABEL_150', 151: 'LABEL_151', 152: 'LABEL_152', 153: 'LABEL_153', 154: 'LABEL_154', 155: 'LABEL_155', 156: 'LABEL_156', 157: 'LABEL_157', 158: 'LABEL_158', 159: 'LABEL_159', 160: 'LABEL_160', 161: 'LABEL_161', 162: 'LABEL_162', 163: 'LABEL_163', 164: 'LABEL_164', 165: 'LABEL_165', 166: 'LABEL_166', 167: 'LABEL_167', 168: 'LABEL_168', 169: 'LABEL_169', 170: 'LABEL_170', 171: 'LABEL_171', 172: 'LABEL_172', 173: 'LABEL_173', 174: 'LABEL_174', 175: 'LABEL_175', 176: 'LABEL_176', 177: 'LABEL_177', 178: 'LABEL_178', 179: 'LABEL_179', 180: 'LABEL_180', 181: 'LABEL_181', 182: 'LABEL_182', 183: 'LABEL_183', 184: 'LABEL_184', 185: 'LABEL_185', 186: 'LABEL_186', 187: 'LABEL_187', 188: 'LABEL_188', 189: 'LABEL_189', 190: 'LABEL_190', 191: 'LABEL_191', 192: 'LABEL_192', 193: 'LABEL_193', 194: 'LABEL_194', 195: 'LABEL_195', 196: 'LABEL_196', 197: 'LABEL_197', 198: 'LABEL_198', 199: 'LABEL_199', 200: 'LABEL_200', 201: 'LABEL_201', 202: 'LABEL_202', 203: 'LABEL_203', 204: 'LABEL_204', 205: 'LABEL_205', 206: 'LABEL_206', 207: 'LABEL_207', 208: 'LABEL_208', 209: 'LABEL_209', 210: 'LABEL_210', 211: 'LABEL_211', 212: 'LABEL_212', 213: 'LABEL_213', 214: 'LABEL_214', 215: 'LABEL_215', 216: 'LABEL_216', 217: 'LABEL_217', 218: 'LABEL_218', 219: 'LABEL_219', 220: 'LABEL_220', 221: 'LABEL_221', 222: 'LABEL_222', 223: 'LABEL_223', 224: 'LABEL_224', 225: 'LABEL_225', 226: 'LABEL_226', 227: 'LABEL_227', 228: 'LABEL_228', 229: 'LABEL_229', 230: 'LABEL_230', 231: 'LABEL_231', 232: 'LABEL_232', 233: 'LABEL_233', 234: 'LABEL_234', 235: 'LABEL_235', 236: 'LABEL_236', 237: 'LABEL_237', 238: 'LABEL_238', 239: 'LABEL_239', 240: 'LABEL_240', 241: 'LABEL_241', 242: 'LABEL_242', 243: 'LABEL_243', 244: 'LABEL_244', 245: 'LABEL_245', 246: 'LABEL_246', 247: 'LABEL_247', 248: 'LABEL_248', 249: 'LABEL_249', 250: 'LABEL_250', 251: 'LABEL_251', 252: 'LABEL_252', 253: 'LABEL_253', 254: 'LABEL_254', 255: 'LABEL_255', 256: 'LABEL_256', 257: 'LABEL_257', 258: 'LABEL_258', 259: 'LABEL_259', 260: 'LABEL_260', 261: 'LABEL_261', 262: 'LABEL_262', 263: 'LABEL_263', 264: 'LABEL_264', 265: 'LABEL_265', 266: 'LABEL_266', 267: 'LABEL_267', 268: 'LABEL_268', 269: 'LABEL_269', 270: 'LABEL_270', 271: 'LABEL_271', 272: 'LABEL_272', 273: 'LABEL_273', 274: 'LABEL_274', 275: 'LABEL_275', 276: 'LABEL_276', 277: 'LABEL_277', 278: 'LABEL_278', 279: 'LABEL_279', 280: 'LABEL_280', 281: 'LABEL_281', 282: 'LABEL_282', 283: 'LABEL_283', 284: 'LABEL_284', 285: 'LABEL_285', 286: 'LABEL_286', 287: 'LABEL_287', 288: 'LABEL_288', 289: 'LABEL_289', 290: 'LABEL_290', 291: 'LABEL_291', 292: 'LABEL_292', 293: 'LABEL_293', 294: 'LABEL_294', 295: 'LABEL_295', 296: 'LABEL_296', 297: 'LABEL_297', 298: 'LABEL_298', 299: 'LABEL_299', 300: 'LABEL_300', 301: 'LABEL_301', 302: 'LABEL_302', 303: 'LABEL_303', 304: 'LABEL_304', 305: 'LABEL_305', 306: 'LABEL_306', 307: 'LABEL_307', 308: 'LABEL_308', 309: 'LABEL_309', 310: 'LABEL_310', 311: 'LABEL_311', 312: 'LABEL_312', 313: 'LABEL_313', 314: 'LABEL_314', 315: 'LABEL_315', 316: 'LABEL_316', 317: 'LABEL_317', 318: 'LABEL_318', 319: 'LABEL_319', 320: 'LABEL_320', 321: 'LABEL_321', 322: 'LABEL_322', 323: 'LABEL_323', 324: 'LABEL_324', 325: 'LABEL_325', 326: 'LABEL_326', 327: 'LABEL_327', 328: 'LABEL_328', 329: 'LABEL_329', 330: 'LABEL_330', 331: 'LABEL_331', 332: 'LABEL_332', 333: 'LABEL_333', 334: 'LABEL_334', 335: 'LABEL_335', 336: 'LABEL_336', 337: 'LABEL_337', 338: 'LABEL_338', 339: 'LABEL_339', 340: 'LABEL_340', 341: 'LABEL_341', 342: 'LABEL_342', 343: 'LABEL_343', 344: 'LABEL_344', 345: 'LABEL_345', 346: 'LABEL_346', 347: 'LABEL_347', 348: 'LABEL_348', 349: 'LABEL_349', 350: 'LABEL_350', 351: 'LABEL_351', 352: 'LABEL_352', 353: 'LABEL_353', 354: 'LABEL_354', 355: 'LABEL_355', 356: 'LABEL_356', 357: 'LABEL_357', 358: 'LABEL_358', 359: 'LABEL_359', 360: 'LABEL_360', 361: 'LABEL_361', 362: 'LABEL_362', 363: 'LABEL_363', 364: 'LABEL_364', 365: 'LABEL_365', 366: 'LABEL_366', 367: 'LABEL_367', 368: 'LABEL_368', 369: 'LABEL_369', 370: 'LABEL_370', 371: 'LABEL_371', 372: 'LABEL_372', 373: 'LABEL_373', 374: 'LABEL_374', 375: 'LABEL_375', 376: 'LABEL_376', 377: 'LABEL_377', 378: 'LABEL_378', 379: 'LABEL_379', 380: 'LABEL_380', 381: 'LABEL_381', 382: 'LABEL_382', 383: 'LABEL_383', 384: 'LABEL_384', 385: 'LABEL_385', 386: 'LABEL_386', 387: 'LABEL_387', 388: 'LABEL_388', 389: 'LABEL_389', 390: 'LABEL_390', 391: 'LABEL_391', 392: 'LABEL_392', 393: 'LABEL_393', 394: 'LABEL_394', 395: 'LABEL_395', 396: 'LABEL_396', 397: 'LABEL_397', 398: 'LABEL_398', 399: 'LABEL_399', 400: 'LABEL_400', 401: 'LABEL_401', 402: 'LABEL_402', 403: 'LABEL_403', 404: 'LABEL_404', 405: 'LABEL_405', 406: 'LABEL_406', 407: 'LABEL_407', 408: 'LABEL_408', 409: 'LABEL_409', 410: 'LABEL_410', 411: 'LABEL_411', 412: 'LABEL_412', 413: 'LABEL_413', 414: 'LABEL_414', 415: 'LABEL_415', 416: 'LABEL_416', 417: 'LABEL_417', 418: 'LABEL_418', 419: 'LABEL_419', 420: 'LABEL_420', 421: 'LABEL_421', 422: 'LABEL_422', 423: 'LABEL_423', 424: 'LABEL_424', 425: 'LABEL_425', 426: 'LABEL_426', 427: 'LABEL_427', 428: 'LABEL_428', 429: 'LABEL_429', 430: 'LABEL_430', 431: 'LABEL_431', 432: 'LABEL_432', 433: 'LABEL_433', 434: 'LABEL_434', 435: 'LABEL_435', 436: 'LABEL_436', 437: 'LABEL_437', 438: 'LABEL_438', 439: 'LABEL_439', 440: 'LABEL_440', 441: 'LABEL_441', 442: 'LABEL_442', 443: 'LABEL_443', 444: 'LABEL_444', 445: 'LABEL_445', 446: 'LABEL_446', 447: 'LABEL_447', 448: 'LABEL_448', 449: 'LABEL_449', 450: 'LABEL_450', 451: 'LABEL_451', 452: 'LABEL_452', 453: 'LABEL_453', 454: 'LABEL_454', 455: 'LABEL_455', 456: 'LABEL_456', 457: 'LABEL_457', 458: 'LABEL_458', 459: 'LABEL_459', 460: 'LABEL_460', 461: 'LABEL_461', 462: 'LABEL_462', 463: 'LABEL_463', 464: 'LABEL_464', 465: 'LABEL_465', 466: 'LABEL_466', 467: 'LABEL_467', 468: 'LABEL_468', 469: 'LABEL_469', 470: 'LABEL_470', 471: 'LABEL_471', 472: 'LABEL_472', 473: 'LABEL_473', 474: 'LABEL_474', 475: 'LABEL_475', 476: 'LABEL_476', 477: 'LABEL_477', 478: 'LABEL_478', 479: 'LABEL_479', 480: 'LABEL_480', 481: 'LABEL_481', 482: 'LABEL_482', 483: 'LABEL_483', 484: 'LABEL_484', 485: 'LABEL_485', 486: 'LABEL_486', 487: 'LABEL_487', 488: 'LABEL_488', 489: 'LABEL_489', 490: 'LABEL_490', 491: 'LABEL_491', 492: 'LABEL_492', 493: 'LABEL_493', 494: 'LABEL_494', 495: 'LABEL_495', 496: 'LABEL_496', 497: 'LABEL_497', 498: 'LABEL_498', 499: 'LABEL_499'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47, 'LABEL_48': 48, 'LABEL_49': 49, 'LABEL_50': 50, 'LABEL_51': 51, 'LABEL_52': 52, 'LABEL_53': 53, 'LABEL_54': 54, 'LABEL_55': 55, 'LABEL_56': 56, 'LABEL_57': 57, 'LABEL_58': 58, 'LABEL_59': 59, 'LABEL_60': 60, 'LABEL_61': 61, 'LABEL_62': 62, 'LABEL_63': 63, 'LABEL_64': 64, 'LABEL_65': 65, 'LABEL_66': 66, 'LABEL_67': 67, 'LABEL_68': 68, 'LABEL_69': 69, 'LABEL_70': 70, 'LABEL_71': 71, 'LABEL_72': 72, 'LABEL_73': 73, 'LABEL_74': 74, 'LABEL_75': 75, 'LABEL_76': 76, 'LABEL_77': 77, 'LABEL_78': 78, 'LABEL_79': 79, 'LABEL_80': 80, 'LABEL_81': 81, 'LABEL_82': 82, 'LABEL_83': 83, 'LABEL_84': 84, 'LABEL_85': 85, 'LABEL_86': 86, 'LABEL_87': 87, 'LABEL_88': 88, 'LABEL_89': 89, 'LABEL_90': 90, 'LABEL_91': 91, 'LABEL_92': 92, 'LABEL_93': 93, 'LABEL_94': 94, 'LABEL_95': 95, 'LABEL_96': 96, 'LABEL_97': 97, 'LABEL_98': 98, 'LABEL_99': 99, 'LABEL_100': 100, 'LABEL_101': 101, 'LABEL_102': 102, 'LABEL_103': 103, 'LABEL_104': 104, 'LABEL_105': 105, 'LABEL_106': 106, 'LABEL_107': 107, 'LABEL_108': 108, 'LABEL_109': 109, 'LABEL_110': 110, 'LABEL_111': 111, 'LABEL_112': 112, 'LABEL_113': 113, 'LABEL_114': 114, 'LABEL_115': 115, 'LABEL_116': 116, 'LABEL_117': 117, 'LABEL_118': 118, 'LABEL_119': 119, 'LABEL_120': 120, 'LABEL_121': 121, 'LABEL_122': 122, 'LABEL_123': 123, 'LABEL_124': 124, 'LABEL_125': 125, 'LABEL_126': 126, 'LABEL_127': 127, 'LABEL_128': 128, 'LABEL_129': 129, 'LABEL_130': 130, 'LABEL_131': 131, 'LABEL_132': 132, 'LABEL_133': 133, 'LABEL_134': 134, 'LABEL_135': 135, 'LABEL_136': 136, 'LABEL_137': 137, 'LABEL_138': 138, 'LABEL_139': 139, 'LABEL_140': 140, 'LABEL_141': 141, 'LABEL_142': 142, 'LABEL_143': 143, 'LABEL_144': 144, 'LABEL_145': 145, 'LABEL_146': 146, 'LABEL_147': 147, 'LABEL_148': 148, 'LABEL_149': 149, 'LABEL_150': 150, 'LABEL_151': 151, 'LABEL_152': 152, 'LABEL_153': 153, 'LABEL_154': 154, 'LABEL_155': 155, 'LABEL_156': 156, 'LABEL_157': 157, 'LABEL_158': 158, 'LABEL_159': 159, 'LABEL_160': 160, 'LABEL_161': 161, 'LABEL_162': 162, 'LABEL_163': 163, 'LABEL_164': 164, 'LABEL_165': 165, 'LABEL_166': 166, 'LABEL_167': 167, 'LABEL_168': 168, 'LABEL_169': 169, 'LABEL_170': 170, 'LABEL_171': 171, 'LABEL_172': 172, 'LABEL_173': 173, 'LABEL_174': 174, 'LABEL_175': 175, 'LABEL_176': 176, 'LABEL_177': 177, 'LABEL_178': 178, 'LABEL_179': 179, 'LABEL_180': 180, 'LABEL_181': 181, 'LABEL_182': 182, 'LABEL_183': 183, 'LABEL_184': 184, 'LABEL_185': 185, 'LABEL_186': 186, 'LABEL_187': 187, 'LABEL_188': 188, 'LABEL_189': 189, 'LABEL_190': 190, 'LABEL_191': 191, 'LABEL_192': 192, 'LABEL_193': 193, 'LABEL_194': 194, 'LABEL_195': 195, 'LABEL_196': 196, 'LABEL_197': 197, 'LABEL_198': 198, 'LABEL_199': 199, 'LABEL_200': 200, 'LABEL_201': 201, 'LABEL_202': 202, 'LABEL_203': 203, 'LABEL_204': 204, 'LABEL_205': 205, 'LABEL_206': 206, 'LABEL_207': 207, 'LABEL_208': 208, 'LABEL_209': 209, 'LABEL_210': 210, 'LABEL_211': 211, 'LABEL_212': 212, 'LABEL_213': 213, 'LABEL_214': 214, 'LABEL_215': 215, 'LABEL_216': 216, 'LABEL_217': 217, 'LABEL_218': 218, 'LABEL_219': 219, 'LABEL_220': 220, 'LABEL_221': 221, 'LABEL_222': 222, 'LABEL_223': 223, 'LABEL_224': 224, 'LABEL_225': 225, 'LABEL_226': 226, 'LABEL_227': 227, 'LABEL_228': 228, 'LABEL_229': 229, 'LABEL_230': 230, 'LABEL_231': 231, 'LABEL_232': 232, 'LABEL_233': 233, 'LABEL_234': 234, 'LABEL_235': 235, 'LABEL_236': 236, 'LABEL_237': 237, 'LABEL_238': 238, 'LABEL_239': 239, 'LABEL_240': 240, 'LABEL_241': 241, 'LABEL_242': 242, 'LABEL_243': 243, 'LABEL_244': 244, 'LABEL_245': 245, 'LABEL_246': 246, 'LABEL_247': 247, 'LABEL_248': 248, 'LABEL_249': 249, 'LABEL_250': 250, 'LABEL_251': 251, 'LABEL_252': 252, 'LABEL_253': 253, 'LABEL_254': 254, 'LABEL_255': 255, 'LABEL_256': 256, 'LABEL_257': 257, 'LABEL_258': 258, 'LABEL_259': 259, 'LABEL_260': 260, 'LABEL_261': 261, 'LABEL_262': 262, 'LABEL_263': 263, 'LABEL_264': 264, 'LABEL_265': 265, 'LABEL_266': 266, 'LABEL_267': 267, 'LABEL_268': 268, 'LABEL_269': 269, 'LABEL_270': 270, 'LABEL_271': 271, 'LABEL_272': 272, 'LABEL_273': 273, 'LABEL_274': 274, 'LABEL_275': 275, 'LABEL_276': 276, 'LABEL_277': 277, 'LABEL_278': 278, 'LABEL_279': 279, 'LABEL_280': 280, 'LABEL_281': 281, 'LABEL_282': 282, 'LABEL_283': 283, 'LABEL_284': 284, 'LABEL_285': 285, 'LABEL_286': 286, 'LABEL_287': 287, 'LABEL_288': 288, 'LABEL_289': 289, 'LABEL_290': 290, 'LABEL_291': 291, 'LABEL_292': 292, 'LABEL_293': 293, 'LABEL_294': 294, 'LABEL_295': 295, 'LABEL_296': 296, 'LABEL_297': 297, 'LABEL_298': 298, 'LABEL_299': 299, 'LABEL_300': 300, 'LABEL_301': 301, 'LABEL_302': 302, 'LABEL_303': 303, 'LABEL_304': 304, 'LABEL_305': 305, 'LABEL_306': 306, 'LABEL_307': 307, 'LABEL_308': 308, 'LABEL_309': 309, 'LABEL_310': 310, 'LABEL_311': 311, 'LABEL_312': 312, 'LABEL_313': 313, 'LABEL_314': 314, 'LABEL_315': 315, 'LABEL_316': 316, 'LABEL_317': 317, 'LABEL_318': 318, 'LABEL_319': 319, 'LABEL_320': 320, 'LABEL_321': 321, 'LABEL_322': 322, 'LABEL_323': 323, 'LABEL_324': 324, 'LABEL_325': 325, 'LABEL_326': 326, 'LABEL_327': 327, 'LABEL_328': 328, 'LABEL_329': 329, 'LABEL_330': 330, 'LABEL_331': 331, 'LABEL_332': 332, 'LABEL_333': 333, 'LABEL_334': 334, 'LABEL_335': 335, 'LABEL_336': 336, 'LABEL_337': 337, 'LABEL_338': 338, 'LABEL_339': 339, 'LABEL_340': 340, 'LABEL_341': 341, 'LABEL_342': 342, 'LABEL_343': 343, 'LABEL_344': 344, 'LABEL_345': 345, 'LABEL_346': 346, 'LABEL_347': 347, 'LABEL_348': 348, 'LABEL_349': 349, 'LABEL_350': 350, 'LABEL_351': 351, 'LABEL_352': 352, 'LABEL_353': 353, 'LABEL_354': 354, 'LABEL_355': 355, 'LABEL_356': 356, 'LABEL_357': 357, 'LABEL_358': 358, 'LABEL_359': 359, 'LABEL_360': 360, 'LABEL_361': 361, 'LABEL_362': 362, 'LABEL_363': 363, 'LABEL_364': 364, 'LABEL_365': 365, 'LABEL_366': 366, 'LABEL_367': 367, 'LABEL_368': 368, 'LABEL_369': 369, 'LABEL_370': 370, 'LABEL_371': 371, 'LABEL_372': 372, 'LABEL_373': 373, 'LABEL_374': 374, 'LABEL_375': 375, 'LABEL_376': 376, 'LABEL_377': 377, 'LABEL_378': 378, 'LABEL_379': 379, 'LABEL_380': 380, 'LABEL_381': 381, 'LABEL_382': 382, 'LABEL_383': 383, 'LABEL_384': 384, 'LABEL_385': 385, 'LABEL_386': 386, 'LABEL_387': 387, 'LABEL_388': 388, 'LABEL_389': 389, 'LABEL_390': 390, 'LABEL_391': 391, 'LABEL_392': 392, 'LABEL_393': 393, 'LABEL_394': 394, 'LABEL_395': 395, 'LABEL_396': 396, 'LABEL_397': 397, 'LABEL_398': 398, 'LABEL_399': 399, 'LABEL_400': 400, 'LABEL_401': 401, 'LABEL_402': 402, 'LABEL_403': 403, 'LABEL_404': 404, 'LABEL_405': 405, 'LABEL_406': 406, 'LABEL_407': 407, 'LABEL_408': 408, 'LABEL_409': 409, 'LABEL_410': 410, 'LABEL_411': 411, 'LABEL_412': 412, 'LABEL_413': 413, 'LABEL_414': 414, 'LABEL_415': 415, 'LABEL_416': 416, 'LABEL_417': 417, 'LABEL_418': 418, 'LABEL_419': 419, 'LABEL_420': 420, 'LABEL_421': 421, 'LABEL_422': 422, 'LABEL_423': 423, 'LABEL_424': 424, 'LABEL_425': 425, 'LABEL_426': 426, 'LABEL_427': 427, 'LABEL_428': 428, 'LABEL_429': 429, 'LABEL_430': 430, 'LABEL_431': 431, 'LABEL_432': 432, 'LABEL_433': 433, 'LABEL_434': 434, 'LABEL_435': 435, 'LABEL_436': 436, 'LABEL_437': 437, 'LABEL_438': 438, 'LABEL_439': 439, 'LABEL_440': 440, 'LABEL_441': 441, 'LABEL_442': 442, 'LABEL_443': 443, 'LABEL_444': 444, 'LABEL_445': 445, 'LABEL_446': 446, 'LABEL_447': 447, 'LABEL_448': 448, 'LABEL_449': 449, 'LABEL_450': 450, 'LABEL_451': 451, 'LABEL_452': 452, 'LABEL_453': 453, 'LABEL_454': 454, 'LABEL_455': 455, 'LABEL_456': 456, 'LABEL_457': 457, 'LABEL_458': 458, 'LABEL_459': 459, 'LABEL_460': 460, 'LABEL_461': 461, 'LABEL_462': 462, 'LABEL_463': 463, 'LABEL_464': 464, 'LABEL_465': 465, 'LABEL_466': 466, 'LABEL_467': 467, 'LABEL_468': 468, 'LABEL_469': 469, 'LABEL_470': 470, 'LABEL_471': 471, 'LABEL_472': 472, 'LABEL_473': 473, 'LABEL_474': 474, 'LABEL_475': 475, 'LABEL_476': 476, 'LABEL_477': 477, 'LABEL_478': 478, 'LABEL_479': 479, 'LABEL_480': 480, 'LABEL_481': 481, 'LABEL_482': 482, 'LABEL_483': 483, 'LABEL_484': 484, 'LABEL_485': 485, 'LABEL_486': 486, 'LABEL_487': 487, 'LABEL_488': 488, 'LABEL_489': 489, 'LABEL_490': 490, 'LABEL_491': 491, 'LABEL_492': 492, 'LABEL_493': 493, 'LABEL_494': 494, 'LABEL_495': 495, 'LABEL_496': 496, 'LABEL_497': 497, 'LABEL_498': 498, 'LABEL_499': 499}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     18\u001b[39m trainer = Trainer(\n\u001b[32m     19\u001b[39m     model=model,\n\u001b[32m     20\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     compute_metrics=compute_metrics,  \u001b[38;5;66;03m# Add metrics if needed\u001b[39;00m\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3822\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m-> \u001b[39m\u001b[32m3822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3823\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3824\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(outputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3825\u001b[39m         )\n\u001b[32m   3826\u001b[39m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[32m   3827\u001b[39m     loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "\n",
    "# Initialize the model (for example, using BERT)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(mlb.classes_))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  # You can use the validation set if available\n",
    "    compute_metrics=compute_metrics,  # Add metrics if needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af6b10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MultiLabelTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     29\u001b[39m training_args = TrainingArguments(\n\u001b[32m     30\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     logging_steps=\u001b[32m100\u001b[39m,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     41\u001b[39m trainer = MultiLabelTrainer(\n\u001b[32m     42\u001b[39m     model=model,\n\u001b[32m     43\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     compute_metrics=compute_metrics,  \u001b[38;5;66;03m# Optional, if you have metrics to compute\u001b[39;00m\n\u001b[32m     47\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[31mTypeError\u001b[39m: MultiLabelTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Make sure the model returns logits and not just predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Ensure the labels are in the correct format (binary vector for multi-label)\n",
    "        if labels is not None:\n",
    "            # BCEWithLogitsLoss works directly with logits (no need to apply sigmoid)\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())  # Convert to float for BCE\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "# Prepare the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=86)  # Adjust num_labels\n",
    "\n",
    "# Prepare the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Optional, if you have metrics to compute\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6498ca5",
   "metadata": {},
   "source": [
    "# en réutilisant code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23ed8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a1a1ba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 55000 examples [01:28, 618.59 examples/s] \n",
      "Generating test split: 2876 examples [00:12, 239.55 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmulti_eurlex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlevel_3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1649\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[39m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, **prepare_splits_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1649\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1001\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1004\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1005\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1006\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1007\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1008\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1487\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1485\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1608\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[39m\n\u001b[32m   1606\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1607\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/datasets_modules/datasets/multi_eurlex/5a12a7463045d4dcb12896b478c09b5a8a131a02b7e7bce059ba7ececc6584ee/multi_eurlex.py:8296\u001b[39m, in \u001b[36mMultiEURLEX._generate_examples\u001b[39m\u001b[34m(self, archive, filepath)\u001b[39m\n\u001b[32m   8290\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m id_, {\n\u001b[32m   8291\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcelex_id\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33mcelex_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   8292\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: {lang: data[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][lang] \u001b[38;5;28;01mfor\u001b[39;00m lang \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.languages},\n\u001b[32m   8293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33meurovoc_concepts\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.config.label_level],\n\u001b[32m   8294\u001b[39m         }\n\u001b[32m   8295\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   8297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/tarfile.py:702\u001b[39m, in \u001b[36m_FileInFile.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m     b[:\u001b[38;5;28mlen\u001b[39m(buf)] = buf\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/tarfile.py:691\u001b[39m, in \u001b[36m_FileInFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m    690\u001b[39m     \u001b[38;5;28mself\u001b[39m.fileobj.seek(offset + (\u001b[38;5;28mself\u001b[39m.position - start))\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     b = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) != length:\n\u001b[32m    693\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[33m\"\u001b[39m\u001b[33munexpected end of data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/tarfile.py:528\u001b[39m, in \u001b[36m_Stream.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28mself\u001b[39m.pos += \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m buf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/tarfile.py:550\u001b[39m, in \u001b[36m_Stream._read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[33m\"\u001b[39m\u001b[33minvalid compressed data\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "load_dataset('multi_eurlex', language='en', label_level='level_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4c95a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiEURLEXConfig.__init__() missing 1 required positional argument: 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmulti_eurlex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabel_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlevel_3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/load.py:1819\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1817\u001b[39m builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n\u001b[32m   1818\u001b[39m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m builder_instance: DatasetBuilder = \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1833\u001b[39m builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[32m   1835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:343\u001b[39m, in \u001b[36mDatasetBuilder.__init__\u001b[39m\u001b[34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m     config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m] = data_dir\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.config_kwargs = config_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m.config_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:583\u001b[39m, in \u001b[36mDatasetBuilder._create_builder_config\u001b[39m\u001b[34m(self, config_name, custom_features, **config_kwargs)\u001b[39m\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVERSION\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.VERSION:\n\u001b[32m    582\u001b[39m         config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.VERSION\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     builder_config = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mBUILDER_CONFIG_CLASS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m# otherwise use the config_kwargs to overwrite the attributes\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    587\u001b[39m     builder_config = copy.deepcopy(builder_config) \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;28;01melse\u001b[39;00m builder_config\n",
      "\u001b[31mTypeError\u001b[39m: MultiEURLEXConfig.__init__() missing 1 required positional argument: 'language'"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('multi_eurlex', language='en', label_level='level_3', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d99644b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:11, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1608\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[39m\n\u001b[32m   1607\u001b[39m _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/datasets_modules/datasets/multi_eurlex/5a12a7463045d4dcb12896b478c09b5a8a131a02b7e7bce059ba7ececc6584ee/multi_eurlex.py:8298\u001b[39m, in \u001b[36mMultiEURLEX._generate_examples\u001b[39m\u001b[34m(self, archive, filepath)\u001b[39m\n\u001b[32m   8297\u001b[39m data = json.loads(row)\n\u001b[32m-> \u001b[39m\u001b[32m8298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   8299\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m id_, {\n\u001b[32m   8300\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcelex_id\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33mcelex_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   8301\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.config.language],\n\u001b[32m   8302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: data[\u001b[33m\"\u001b[39m\u001b[33meurovoc_concepts\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.config.label_level],\n\u001b[32m   8303\u001b[39m     }\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmulti_eurlex\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mde\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlevel_3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1649\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[39m\n\u001b[32m   1648\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, **prepare_splits_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1649\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1001\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1004\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1005\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1006\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1007\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1008\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1487\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1485\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/datasets/builder.py:1644\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e.__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1643\u001b[39m         e = e.__context__\n\u001b[32m-> \u001b[39m\u001b[32m1644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "load_dataset('multi_eurlex', language=['fr','de','pl','fi'], label_level='level_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075381d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_dataset = load_dataset('multi_eurlex', language='all_languages',\n",
    "                                languages=['fr','de','pl','fi'], label_level='level_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c910a3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['celex_id', 'text', 'labels'],\n",
       "    num_rows: 55000\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
