{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7711086d",
   "metadata": {},
   "source": [
    "# Legal document classification in zero-shot cross lingual transfer setting\n",
    "\n",
    "# Part II: Results reproduction\n",
    "\n",
    "Date: May 2025\n",
    "\n",
    "Project of course: Natural Language Processing - ENSAE 3A S2\n",
    "\n",
    "Author: Noémie Guibé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a2c57bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532a2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 06:30:59.300984: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 06:30:59.400575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745821859.511197  128579 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745821859.520609  128579 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745821859.599597  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599642  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599646  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745821859.599649  128579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-28 06:30:59.608366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5477d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NLP-Legal-document-classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNLP-Legal-document-classification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'NLP-Legal-document-classification'"
     ]
    }
   ],
   "source": [
    "os.chdir('NLP-Legal-document-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39096caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data base\n",
    "df = pd.read_parquet('data/dataset/multi_eurlex_reduced.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703bb25",
   "metadata": {},
   "source": [
    "# Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only level 3 labels\n",
    "df['level_3_labels'] = df['eurovoc_concepts'].apply(lambda d: d['level_3'] if 'level_3' in d else [])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb79671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     celex_id publication_date  \\\n",
      "0  32006D0213       2006-03-06   \n",
      "1  32003R1330       2003-07-25   \n",
      "2  32003R1786       2003-09-29   \n",
      "3  31985R2590       1985-09-13   \n",
      "4  31993R1103       1993-04-30   \n",
      "\n",
      "                                                text  \\\n",
      "0  COMMISSION DECISION\\nof 6 March 2006\\nestablis...   \n",
      "1  Commission Regulation (EC) No 1330/2003\\nof 25...   \n",
      "2  Council Regulation (EC) No 1786/2003\\nof 29 Se...   \n",
      "3  *****\\nCOMMISSION REGULATION (EEC) No 2590/85\\...   \n",
      "4  COMMISSION REGULATION (EEC) No 1103/93 of 30 A...   \n",
      "\n",
      "                                    eurovoc_concepts  split  \\\n",
      "0  {'all_levels': ['1706', '1826', '2754', '3690'...  train   \n",
      "1  {'all_levels': ['1117', '1118', '1605', '2635'...  train   \n",
      "2  {'all_levels': ['2173', '4854', '614', '797'],...  train   \n",
      "3  {'all_levels': ['1201', '1261', '5334', '755',...  train   \n",
      "4  {'all_levels': ['1309', '2159', '2192', '235',...  train   \n",
      "\n",
      "                                         doc_lengths  max_doc_length  \\\n",
      "0  {'de': 3302, 'en': 3233, 'fi': 3073.0, 'fr': 3...            3642   \n",
      "1  {'de': 1430, 'en': 1328, 'fi': 1366.0, 'fr': 1...            1437   \n",
      "2  {'de': 19641, 'en': 17741, 'fi': 17298.0, 'fr'...           19641   \n",
      "3  {'de': 2720, 'en': 2525, 'fi': 2527.0, 'fr': 2...            2720   \n",
      "4  {'de': 29436, 'en': 27992, 'fi': None, 'fr': 3...           32297   \n",
      "\n",
      "                                      level_3_labels  \n",
      "0                [1386, 2825, 138, 2475, 3879, 3641]  \n",
      "1                                 [1115, 2656, 1602]  \n",
      "2                             [614, 712, 1277, 2443]  \n",
      "3                      [2413, 712, 2477, 4488, 2443]  \n",
      "4  [539, 956, 1847, 2106, 614, 2858, 6205, 1845, ...  \n"
     ]
    }
   ],
   "source": [
    "train_df = df[df['split']=='train']\n",
    "# English-only training set\n",
    "train_df.loc[:,'text'] = train_df[\"text\"].apply(lambda x: isinstance(x, dict) and x.get(\"en\"))\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5970fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     celex_id publication_date  \\\n",
      "0  32013R1390       2013-12-16   \n",
      "1  32015R0176       2015-02-05   \n",
      "2  32015R0596       2015-04-15   \n",
      "3  32015D0041       2014-12-17   \n",
      "4  32013D0785       2013-12-16   \n",
      "\n",
      "                                                text  \\\n",
      "0  RÈGLEMENT (UE) No 1390/2013 DU CONSEIL\\ndu 16 ...   \n",
      "1  RÈGLEMENT D'EXÉCUTION (UE) 2015/176 DE LA COMM...   \n",
      "2  RÈGLEMENT D'EXÉCUTION (UE) 2015/596 DE LA COMM...   \n",
      "3  DÉCISION (UE) 2015/41 DU PARLEMENT EUROPÉEN ET...   \n",
      "4  DÉCISION DU CONSEIL\\ndu 16 décembre 2013\\nrela...   \n",
      "\n",
      "                                    eurovoc_concepts split  \\\n",
      "0  {'all_levels': ['1085', '1474', '2329', '2556'...  test   \n",
      "1  {'all_levels': ['2749', '3173', '5573', '5898'...  test   \n",
      "2  {'all_levels': ['1239', '1318', '1878', '2972'...  test   \n",
      "3  {'all_levels': ['1361', '1647', '2543', '2910'...  test   \n",
      "4  {'all_levels': ['1474', '1819', '2329', '2556'...  test   \n",
      "\n",
      "                                         doc_lengths  max_doc_length  \\\n",
      "0  {'de': 3728, 'en': 3410, 'fi': 3364.0, 'fr': 3...            3728   \n",
      "1  {'de': 1847, 'en': 1790, 'fi': 1694.0, 'fr': 1...            1914   \n",
      "2  {'de': 3098, 'en': 2907, 'fi': 2907.0, 'fr': 3...            3111   \n",
      "3  {'de': 3238, 'en': 3041, 'fi': 2933.0, 'fr': 3...            3238   \n",
      "4  {'de': 3019, 'en': 2899, 'fi': 2712.0, 'fr': 2...            3019   \n",
      "\n",
      "                                      level_3_labels lang  \n",
      "0  [3461, 5424, 5283, 122, 2106, 4590, 913, 4040,...   fr  \n",
      "1  [13, 138, 5781, 5283, 122, 87, 2106, 914, 2200...   fr  \n",
      "2  [1329, 5283, 122, 3810, 6052, 2106, 2972, 4590...   fr  \n",
      "3  [5655, 1410, 5283, 5781, 5087, 122, 335, 2106,...   fr  \n",
      "4                            [3461, 4040, 311, 2476]   fr  \n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "test_df = df[df['split']=='test']\n",
    "\n",
    "# Test set in multiple languages\n",
    "test_langs = [\"fr\", \"de\", \"pl\",'fi'] \n",
    "test_dfs = []\n",
    "\n",
    "for lang in test_langs:\n",
    "    # Filter rows where the language exists in the text dictionary\n",
    "    df_lang = test_df[test_df[\"text\"].apply(lambda x: isinstance(x, dict) and lang in x)]\n",
    "    \n",
    "    # Now extract the respective language text, and add the 'lang' column\n",
    "    df_lang.loc[:,\"text\"] = df_lang[\"text\"].apply(lambda x: x[lang])  # Extract the language text\n",
    "    df_lang[\"lang\"] = lang  # Add a new column for language\n",
    "    \n",
    "    # Append to test_dfs\n",
    "    test_dfs.append(df_lang)\n",
    "\n",
    "# Combine the list of DataFrames into one (exploded test set)\n",
    "final_test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(final_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8ccdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128579/3383169465.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(train_df[\"level_3_labels\"])]\n"
     ]
    }
   ],
   "source": [
    "# Fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(df[\"level_3_labels\"])\n",
    "\n",
    "# Now safely transform\n",
    "train_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(train_df[\"level_3_labels\"])]\n",
    "final_test_df[\"label_vector\"] = [row.tolist() for row in mlb.transform(final_test_df[\"level_3_labels\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca1fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active labels for row 5:\n",
      "Index: 90, Label: 1810\n",
      "Index: 173, Label: 2487\n",
      "Index: 270, Label: 3489\n",
      "Index: 338, Label: 4314\n",
      "Index: 450, Label: 614\n"
     ]
    }
   ],
   "source": [
    "row_index = 5  # Change to the row you want to inspect\n",
    "label_vector = train_df[\"label_vector\"].iloc[row_index]\n",
    "\n",
    "active_labels = [\n",
    "    (i, mlb.classes_[i]) for i, val in enumerate(label_vector) if val == 1\n",
    "]\n",
    "\n",
    "print(f\"Active labels for row {row_index}:\")\n",
    "for idx, label in active_labels:\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f08371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test set with label 1810: [32, 181, 381, 480, 619, 708, 881, 940, 964, 1338, 1412, 1807, 2484, 2921, 3125, 3203, 3603, 3620, 3860, 4167, 4491, 4911, 4949, 5028, 5177, 5377, 5476, 5615, 5704, 5877, 5936, 5960, 6334, 6408, 6803, 7480, 7917, 8121, 8199, 8599, 8616, 8856, 9163, 9487, 9907, 9945, 10024, 10173, 10373, 10472, 10611, 10700, 10873, 10932, 10956, 11330, 11404, 11799, 12476, 12913, 13117, 13195, 13595, 13612, 13852, 14159, 14483, 14903, 14941, 15020, 15169, 15369, 15468, 15607, 15696, 15869, 15928, 15952, 16326, 16400, 16795, 17472, 17909, 18113, 18191, 18591, 18608, 18848, 19155, 19479, 19899, 19937]\n"
     ]
    }
   ],
   "source": [
    "# check same index for same label in test dataset\n",
    "label_id = \"1810\"\n",
    "label_index = list(mlb.classes_).index(label_id)\n",
    "\n",
    "# Get row indices in the test set where label 1810 is present\n",
    "matching_indices = [\n",
    "    i for i, row in enumerate(final_test_df[\"label_vector\"]) if row[label_index] == 1\n",
    "]\n",
    "\n",
    "print(f\"Rows in test set with label {label_id}: {matching_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6780e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0dc64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active labels for row 32:\n",
      "Index: 90, Label: 1810\n",
      "Index: 128, Label: 2188\n",
      "Index: 265, Label: 3461\n"
     ]
    }
   ],
   "source": [
    "row_index = 32  # Change to the row you want to inspect\n",
    "label_vector = final_test_df[\"label_vector\"].iloc[row_index]\n",
    "\n",
    "active_labels = [\n",
    "    (i, mlb.classes_[i]) for i, val in enumerate(label_vector) if val == 1\n",
    "]\n",
    "\n",
    "print(f\"Active labels for row {row_index}:\")\n",
    "for idx, label in active_labels:\n",
    "    print(f\"Index: {idx}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2235c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label_vector\"]])\n",
    "test_datasets = {\n",
    "    lang: Dataset.from_pandas(df[[\"text\", \"label_vector\"]]) \n",
    "    for lang, df in final_test_df.groupby(\"lang\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3541637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54994/54994 [04:04<00:00, 224.75 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:43<00:00, 115.02 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:38<00:00, 129.08 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:43<00:00, 114.32 examples/s]\n",
      "Map: 100%|██████████| 4996/4996 [00:39<00:00, 125.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "def tokenize_and_format_tf(batch):\n",
    "    encodings = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    encodings['labels'] = batch['label_vector']\n",
    "    return encodings\n",
    "\n",
    "# Tokenize the datasets (train and test)\n",
    "train_dataset = train_dataset.map(tokenize_and_format_tf, batched=True)\n",
    "for lang in test_datasets:\n",
    "    test_datasets[lang] = test_datasets[lang].map(tokenize_and_format_tf, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c86f97f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMMISSION DECISION\\nof 6 March 2006\\nestablishing the classes of reaction-to-fire performance for certain construction products as regards wood flooring and solid wood panelling and cladding\\n(notified under document number C(2006) 655)\\n(Text with EEA relevance)\\n(2006/213/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Directive 89/106/EEC of 21 December 1988, on the approximation of laws, regulations and administrative provisions of the Member States relating to construction products (1), and in particular Article 20(2) thereof,\\nWhereas:\\n(1)\\nDirective 89/106/EEC envisages that in order to take account of different levels of protection for construction works at national, regional or local level, it may be necessary to establish in the interpretative documents classes corresponding to the performance of products in respect of each essential requirement. Those documents have been published as the ‘Communication of the Commission with regard to the interpretative documents of Directive 89/106/EEC’ (2).\\n(2)\\nWith respect to the essential requirement of safety in the event of fire, interpretative document No 2 lists a number of interrelated measures which together define the fire safety strategy to be variously developed in the Member States.\\n(3)\\nInterpretative document No 2 identifies one of those measures as the limitation of the generation and spread of fire and smoke within a given area by limiting the potential of construction products to contribute to the full development of a fire.\\n(4)\\nThe level of that limitation may be expressed only in terms of the different levels of reaction-to-fire performance of the products in their end-use application;\\n(5)\\nBy way of harmonised solution, a system of classes was adopted in Commission Decision 2000/147/EC of 8 February 2000 implementing Council Directive 89/106/EEC as regards the classification of the reaction-to-fire performance of construction products (3).\\n(6)\\nIn the case of wood flooring and solid wood panelling and cladding it is necessary to use the classification established in Decision 2000/147/EC.\\n(7)\\nThe reaction-to-fire performance of many construction products and/or materials, within the classification provided for in Decision 2000/147/EC, is well established and sufficiently well known to fire regulators in Member States that they do not require testing for this particular performance characteristic.\\n(8)\\nThe measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Construction,\\nHAS ADOPTED THIS DECISION:\\nArticle 1\\nThe construction products and/or materials which satisfy all the requirements of the performance characteristic ‘reaction to fire’ without need for further testing are set out in the Annex.\\nArticle 2\\nThe specific classes to be applied to different construction products and/or materials, within the reaction-to-fire classification adopted in Decision 2000/147/EC, are set out in the Annex to this Decision.\\nArticle 3\\nProducts shall be considered in relation to their end-use application, where relevant.\\nArticle 4\\nThis Decision is addressed to the Member States.\\nDone at Brussels, 6 March 2006.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99cb700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 31436,\n",
       " 230498,\n",
       " 1514,\n",
       " 17304,\n",
       " 118596,\n",
       " 111,\n",
       " 305,\n",
       " 11994,\n",
       " 3295,\n",
       " 137633,\n",
       " 214,\n",
       " 70,\n",
       " 61112,\n",
       " 111,\n",
       " 132539,\n",
       " 9,\n",
       " 188,\n",
       " 9,\n",
       " 73702,\n",
       " 23718,\n",
       " 100,\n",
       " 24233,\n",
       " 50961,\n",
       " 38742,\n",
       " 237,\n",
       " 28601,\n",
       " 7,\n",
       " 109412,\n",
       " 74912,\n",
       " 214,\n",
       " 136,\n",
       " 18652,\n",
       " 109412,\n",
       " 16138,\n",
       " 2069,\n",
       " 136,\n",
       " 26580,\n",
       " 59725,\n",
       " 15,\n",
       " 10869,\n",
       " 47314,\n",
       " 1379,\n",
       " 12937,\n",
       " 14012,\n",
       " 313,\n",
       " 132,\n",
       " 22435,\n",
       " 16,\n",
       " 305,\n",
       " 70265,\n",
       " 15,\n",
       " 174379,\n",
       " 678,\n",
       " 241,\n",
       " 24040,\n",
       " 89515,\n",
       " 329,\n",
       " 16,\n",
       " 15,\n",
       " 22435,\n",
       " 12477,\n",
       " 2681,\n",
       " 64,\n",
       " 16546,\n",
       " 16,\n",
       " 23373,\n",
       " 31436,\n",
       " 230498,\n",
       " 31766,\n",
       " 23373,\n",
       " 206713,\n",
       " 9330,\n",
       " 31436,\n",
       " 594,\n",
       " 65761,\n",
       " 179904,\n",
       " 4,\n",
       " 171855,\n",
       " 28601,\n",
       " 47,\n",
       " 70,\n",
       " 4804,\n",
       " 67530,\n",
       " 137633,\n",
       " 214,\n",
       " 70,\n",
       " 28811,\n",
       " 66069,\n",
       " 4,\n",
       " 171855,\n",
       " 28601,\n",
       " 47,\n",
       " 42666,\n",
       " 5844,\n",
       " 17853,\n",
       " 64,\n",
       " 54100,\n",
       " 64,\n",
       " 24821,\n",
       " 441,\n",
       " 111,\n",
       " 952,\n",
       " 14487,\n",
       " 18592,\n",
       " 4,\n",
       " 98,\n",
       " 70,\n",
       " 35707,\n",
       " 53950,\n",
       " 2320,\n",
       " 111,\n",
       " 131703,\n",
       " 4,\n",
       " 209332,\n",
       " 136,\n",
       " 86757,\n",
       " 125034,\n",
       " 7,\n",
       " 111,\n",
       " 70,\n",
       " 74057,\n",
       " 46684,\n",
       " 33444,\n",
       " 214,\n",
       " 47,\n",
       " 50961,\n",
       " 38742,\n",
       " 798,\n",
       " 4,\n",
       " 136,\n",
       " 23,\n",
       " 17311,\n",
       " 24241,\n",
       " 387,\n",
       " 40970,\n",
       " 2685,\n",
       " 4390,\n",
       " 4,\n",
       " 78662,\n",
       " 162,\n",
       " 12,\n",
       " 798,\n",
       " 42666,\n",
       " 5844,\n",
       " 17853,\n",
       " 64,\n",
       " 54100,\n",
       " 64,\n",
       " 24821,\n",
       " 441,\n",
       " 6,\n",
       " 191195,\n",
       " 7,\n",
       " 450,\n",
       " 23,\n",
       " 12989,\n",
       " 47,\n",
       " 5646,\n",
       " 15426,\n",
       " 111,\n",
       " 12921,\n",
       " 90926,\n",
       " 111,\n",
       " 48431,\n",
       " 100,\n",
       " 50961,\n",
       " 43240,\n",
       " 99,\n",
       " 15889,\n",
       " 4,\n",
       " 18150,\n",
       " 707,\n",
       " 4000,\n",
       " 17366,\n",
       " 4,\n",
       " 442,\n",
       " 1543,\n",
       " 186,\n",
       " 63559,\n",
       " 47,\n",
       " 137633,\n",
       " 23,\n",
       " 70,\n",
       " 26389,\n",
       " 4935,\n",
       " 60525,\n",
       " 61112,\n",
       " 42518,\n",
       " 214,\n",
       " 47,\n",
       " 70,\n",
       " 23718,\n",
       " 111,\n",
       " 38742,\n",
       " 23,\n",
       " 15072,\n",
       " 111,\n",
       " 12638,\n",
       " 85590,\n",
       " 64209,\n",
       " 674,\n",
       " 5,\n",
       " 139661,\n",
       " 60525,\n",
       " 765,\n",
       " 2809,\n",
       " 91376,\n",
       " 237,\n",
       " 70,\n",
       " 204,\n",
       " 166697,\n",
       " 14,\n",
       " 120639,\n",
       " 111,\n",
       " 70,\n",
       " 63871,\n",
       " 678,\n",
       " 28601,\n",
       " 47,\n",
       " 70,\n",
       " 26389,\n",
       " 4935,\n",
       " 60525,\n",
       " 111,\n",
       " 42666,\n",
       " 5844,\n",
       " 17853,\n",
       " 64,\n",
       " 54100,\n",
       " 64,\n",
       " 24821,\n",
       " 441,\n",
       " 26,\n",
       " 1737,\n",
       " 5,\n",
       " 1737,\n",
       " 17106,\n",
       " 15072,\n",
       " 47,\n",
       " 70,\n",
       " 85590,\n",
       " 64209,\n",
       " 674,\n",
       " 111,\n",
       " 81900,\n",
       " 23,\n",
       " 70,\n",
       " 19732,\n",
       " 111,\n",
       " 11476,\n",
       " 4,\n",
       " 26389,\n",
       " 4935,\n",
       " 12937,\n",
       " 438,\n",
       " 116,\n",
       " 5303,\n",
       " 7,\n",
       " 10,\n",
       " 14012,\n",
       " 111,\n",
       " 1940,\n",
       " 174822,\n",
       " 72350,\n",
       " 7,\n",
       " 3129,\n",
       " 25842,\n",
       " 61924,\n",
       " 70,\n",
       " 11476,\n",
       " 81900,\n",
       " 113857,\n",
       " 47,\n",
       " 186,\n",
       " 67842,\n",
       " 538,\n",
       " 126809,\n",
       " 23,\n",
       " 70,\n",
       " 74057,\n",
       " 46684,\n",
       " 5,\n",
       " 2788,\n",
       " 159838,\n",
       " 4935,\n",
       " 12937,\n",
       " 438,\n",
       " 116,\n",
       " 6,\n",
       " 42485,\n",
       " 90,\n",
       " 1632,\n",
       " 111,\n",
       " 8382,\n",
       " 72350,\n",
       " 7,\n",
       " 237,\n",
       " 70,\n",
       " 205969,\n",
       " 111,\n",
       " 70,\n",
       " 58093,\n",
       " 136,\n",
       " 93403,\n",
       " 111,\n",
       " 11476,\n",
       " 136,\n",
       " 2614,\n",
       " 350,\n",
       " 28032,\n",
       " 10,\n",
       " 34475,\n",
       " 16128,\n",
       " 390,\n",
       " 17475,\n",
       " 214,\n",
       " 70,\n",
       " 38516,\n",
       " 111,\n",
       " 50961,\n",
       " 38742,\n",
       " 47,\n",
       " 162466,\n",
       " 47,\n",
       " 70,\n",
       " 4393,\n",
       " 34754,\n",
       " 111,\n",
       " 10,\n",
       " 11476,\n",
       " 5,\n",
       " 3971,\n",
       " 581,\n",
       " 17366,\n",
       " 111,\n",
       " 450,\n",
       " 205969,\n",
       " 1543,\n",
       " 186,\n",
       " 36510,\n",
       " 297,\n",
       " 4734,\n",
       " 23,\n",
       " 69407,\n",
       " 111,\n",
       " 70,\n",
       " 12921,\n",
       " 90926,\n",
       " 111,\n",
       " 132539,\n",
       " 9,\n",
       " 188,\n",
       " 9,\n",
       " 73702,\n",
       " 23718,\n",
       " 111,\n",
       " 70,\n",
       " 38742,\n",
       " 23,\n",
       " 2363,\n",
       " 3564,\n",
       " 9,\n",
       " 4032,\n",
       " 38415,\n",
       " 74,\n",
       " 5211,\n",
       " 3311,\n",
       " 3917,\n",
       " 111,\n",
       " 22313,\n",
       " 5281,\n",
       " 29806,\n",
       " 4,\n",
       " 10,\n",
       " 5426,\n",
       " 111,\n",
       " 61112,\n",
       " 509,\n",
       " 30666,\n",
       " 297,\n",
       " 23,\n",
       " 63871,\n",
       " 49132,\n",
       " 6889,\n",
       " 3576,\n",
       " 64,\n",
       " 128497,\n",
       " 64,\n",
       " 16546,\n",
       " 111,\n",
       " 382,\n",
       " 22482,\n",
       " 3576,\n",
       " 29479,\n",
       " 214,\n",
       " 70615,\n",
       " 42666,\n",
       " 5844,\n",
       " 17853,\n",
       " 64,\n",
       " 54100,\n",
       " 64,\n",
       " 24821,\n",
       " 441,\n",
       " 237,\n",
       " 28601,\n",
       " 7,\n",
       " 70,\n",
       " 40865,\n",
       " 1363,\n",
       " 111,\n",
       " 70,\n",
       " 132539,\n",
       " 9,\n",
       " 188,\n",
       " 9,\n",
       " 73702,\n",
       " 23718,\n",
       " 111,\n",
       " 50961,\n",
       " 38742,\n",
       " 2788,\n",
       " 5,\n",
       " 6791,\n",
       " 360,\n",
       " 70,\n",
       " 7225,\n",
       " 111,\n",
       " 109412,\n",
       " 74912,\n",
       " 214,\n",
       " 136,\n",
       " 18652,\n",
       " 109412,\n",
       " 16138,\n",
       " 2069,\n",
       " 136,\n",
       " 26580,\n",
       " 59725,\n",
       " 442,\n",
       " 83,\n",
       " 63559,\n",
       " 47,\n",
       " 4527,\n",
       " 70,\n",
       " 40865,\n",
       " 1363,\n",
       " 170920,\n",
       " 23,\n",
       " 49132,\n",
       " 6889,\n",
       " 3576,\n",
       " 64,\n",
       " 128497,\n",
       " 64,\n",
       " 16546,\n",
       " 5,\n",
       " 8696,\n",
       " 581,\n",
       " 132539,\n",
       " 9,\n",
       " 188,\n",
       " 9,\n",
       " 73702,\n",
       " 23718,\n",
       " 111,\n",
       " 5941,\n",
       " 50961,\n",
       " 38742,\n",
       " 136,\n",
       " 64,\n",
       " 748,\n",
       " 76319,\n",
       " 4,\n",
       " 28032,\n",
       " 70,\n",
       " 40865,\n",
       " 1363,\n",
       " 62952,\n",
       " 100,\n",
       " 23,\n",
       " 49132,\n",
       " 6889,\n",
       " 3576,\n",
       " 64,\n",
       " 128497,\n",
       " 64,\n",
       " 16546,\n",
       " 4,\n",
       " 83,\n",
       " 5299,\n",
       " 170920,\n",
       " 136,\n",
       " 6,\n",
       " 129980,\n",
       " 538,\n",
       " 5299,\n",
       " 51529,\n",
       " 47,\n",
       " 11476,\n",
       " 144314,\n",
       " 7,\n",
       " 23,\n",
       " 74057,\n",
       " 46684,\n",
       " 450,\n",
       " 1836,\n",
       " 54,\n",
       " 959,\n",
       " 64209,\n",
       " 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba7d14d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁COM',\n",
       " 'MISSION',\n",
       " '▁DE',\n",
       " 'CI',\n",
       " 'SION',\n",
       " '▁of',\n",
       " '▁6',\n",
       " '▁March',\n",
       " '▁2006',\n",
       " '▁establish',\n",
       " 'ing',\n",
       " '▁the',\n",
       " '▁classes',\n",
       " '▁of',\n",
       " '▁reaction',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'fire',\n",
       " '▁performance',\n",
       " '▁for',\n",
       " '▁certain',\n",
       " '▁construction',\n",
       " '▁products',\n",
       " '▁as',\n",
       " '▁regard',\n",
       " 's',\n",
       " '▁wood',\n",
       " '▁floor',\n",
       " 'ing',\n",
       " '▁and',\n",
       " '▁solid',\n",
       " '▁wood',\n",
       " '▁panel',\n",
       " 'ling',\n",
       " '▁and',\n",
       " '▁cla',\n",
       " 'dding',\n",
       " '▁(',\n",
       " 'not',\n",
       " 'ified',\n",
       " '▁under',\n",
       " '▁document',\n",
       " '▁number',\n",
       " '▁C',\n",
       " '(',\n",
       " '2006',\n",
       " ')',\n",
       " '▁6',\n",
       " '55)',\n",
       " '▁(',\n",
       " 'Text',\n",
       " '▁with',\n",
       " '▁E',\n",
       " 'EA',\n",
       " '▁relevan',\n",
       " 'ce',\n",
       " ')',\n",
       " '▁(',\n",
       " '2006',\n",
       " '/2',\n",
       " '13',\n",
       " '/',\n",
       " 'EC',\n",
       " ')',\n",
       " '▁THE',\n",
       " '▁COM',\n",
       " 'MISSION',\n",
       " '▁OF',\n",
       " '▁THE',\n",
       " '▁EUROPE',\n",
       " 'AN',\n",
       " '▁COM',\n",
       " 'M',\n",
       " 'UNI',\n",
       " 'TIES',\n",
       " ',',\n",
       " '▁Having',\n",
       " '▁regard',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁Tre',\n",
       " 'aty',\n",
       " '▁establish',\n",
       " 'ing',\n",
       " '▁the',\n",
       " '▁European',\n",
       " '▁Community',\n",
       " ',',\n",
       " '▁Having',\n",
       " '▁regard',\n",
       " '▁to',\n",
       " '▁Direct',\n",
       " 'ive',\n",
       " '▁89',\n",
       " '/',\n",
       " '106',\n",
       " '/',\n",
       " 'EE',\n",
       " 'C',\n",
       " '▁of',\n",
       " '▁21',\n",
       " '▁December',\n",
       " '▁1988',\n",
       " ',',\n",
       " '▁on',\n",
       " '▁the',\n",
       " '▁appro',\n",
       " 'xim',\n",
       " 'ation',\n",
       " '▁of',\n",
       " '▁laws',\n",
       " ',',\n",
       " '▁regulations',\n",
       " '▁and',\n",
       " '▁administrative',\n",
       " '▁provision',\n",
       " 's',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁Member',\n",
       " '▁States',\n",
       " '▁relat',\n",
       " 'ing',\n",
       " '▁to',\n",
       " '▁construction',\n",
       " '▁products',\n",
       " '▁(1)',\n",
       " ',',\n",
       " '▁and',\n",
       " '▁in',\n",
       " '▁particular',\n",
       " '▁Article',\n",
       " '▁20',\n",
       " '(2)',\n",
       " '▁there',\n",
       " 'of',\n",
       " ',',\n",
       " '▁Where',\n",
       " 'as',\n",
       " ':',\n",
       " '▁(1)',\n",
       " '▁Direct',\n",
       " 'ive',\n",
       " '▁89',\n",
       " '/',\n",
       " '106',\n",
       " '/',\n",
       " 'EE',\n",
       " 'C',\n",
       " '▁',\n",
       " 'envisage',\n",
       " 's',\n",
       " '▁that',\n",
       " '▁in',\n",
       " '▁order',\n",
       " '▁to',\n",
       " '▁take',\n",
       " '▁account',\n",
       " '▁of',\n",
       " '▁different',\n",
       " '▁levels',\n",
       " '▁of',\n",
       " '▁protection',\n",
       " '▁for',\n",
       " '▁construction',\n",
       " '▁works',\n",
       " '▁at',\n",
       " '▁national',\n",
       " ',',\n",
       " '▁regional',\n",
       " '▁or',\n",
       " '▁local',\n",
       " '▁level',\n",
       " ',',\n",
       " '▁it',\n",
       " '▁may',\n",
       " '▁be',\n",
       " '▁necessary',\n",
       " '▁to',\n",
       " '▁establish',\n",
       " '▁in',\n",
       " '▁the',\n",
       " '▁interpreta',\n",
       " 'tive',\n",
       " '▁documents',\n",
       " '▁classes',\n",
       " '▁correspond',\n",
       " 'ing',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁performance',\n",
       " '▁of',\n",
       " '▁products',\n",
       " '▁in',\n",
       " '▁respect',\n",
       " '▁of',\n",
       " '▁each',\n",
       " '▁essential',\n",
       " '▁require',\n",
       " 'ment',\n",
       " '.',\n",
       " '▁Those',\n",
       " '▁documents',\n",
       " '▁have',\n",
       " '▁been',\n",
       " '▁published',\n",
       " '▁as',\n",
       " '▁the',\n",
       " '▁‘',\n",
       " 'Commun',\n",
       " 'i',\n",
       " 'cation',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁Commission',\n",
       " '▁with',\n",
       " '▁regard',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁interpreta',\n",
       " 'tive',\n",
       " '▁documents',\n",
       " '▁of',\n",
       " '▁Direct',\n",
       " 'ive',\n",
       " '▁89',\n",
       " '/',\n",
       " '106',\n",
       " '/',\n",
       " 'EE',\n",
       " 'C',\n",
       " '’',\n",
       " '▁(2)',\n",
       " '.',\n",
       " '▁(2)',\n",
       " '▁With',\n",
       " '▁respect',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁essential',\n",
       " '▁require',\n",
       " 'ment',\n",
       " '▁of',\n",
       " '▁safety',\n",
       " '▁in',\n",
       " '▁the',\n",
       " '▁event',\n",
       " '▁of',\n",
       " '▁fire',\n",
       " ',',\n",
       " '▁interpreta',\n",
       " 'tive',\n",
       " '▁document',\n",
       " '▁No',\n",
       " '▁2',\n",
       " '▁list',\n",
       " 's',\n",
       " '▁a',\n",
       " '▁number',\n",
       " '▁of',\n",
       " '▁inter',\n",
       " 'related',\n",
       " '▁measure',\n",
       " 's',\n",
       " '▁which',\n",
       " '▁together',\n",
       " '▁define',\n",
       " '▁the',\n",
       " '▁fire',\n",
       " '▁safety',\n",
       " '▁strategy',\n",
       " '▁to',\n",
       " '▁be',\n",
       " '▁various',\n",
       " 'ly',\n",
       " '▁developed',\n",
       " '▁in',\n",
       " '▁the',\n",
       " '▁Member',\n",
       " '▁States',\n",
       " '.',\n",
       " '▁(3)',\n",
       " '▁Interpreta',\n",
       " 'tive',\n",
       " '▁document',\n",
       " '▁No',\n",
       " '▁2',\n",
       " '▁',\n",
       " 'identifi',\n",
       " 'es',\n",
       " '▁one',\n",
       " '▁of',\n",
       " '▁those',\n",
       " '▁measure',\n",
       " 's',\n",
       " '▁as',\n",
       " '▁the',\n",
       " '▁limitation',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁generation',\n",
       " '▁and',\n",
       " '▁spread',\n",
       " '▁of',\n",
       " '▁fire',\n",
       " '▁and',\n",
       " '▁smo',\n",
       " 'ke',\n",
       " '▁within',\n",
       " '▁a',\n",
       " '▁given',\n",
       " '▁area',\n",
       " '▁by',\n",
       " '▁limit',\n",
       " 'ing',\n",
       " '▁the',\n",
       " '▁potential',\n",
       " '▁of',\n",
       " '▁construction',\n",
       " '▁products',\n",
       " '▁to',\n",
       " '▁contribute',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁full',\n",
       " '▁development',\n",
       " '▁of',\n",
       " '▁a',\n",
       " '▁fire',\n",
       " '.',\n",
       " '▁(4)',\n",
       " '▁The',\n",
       " '▁level',\n",
       " '▁of',\n",
       " '▁that',\n",
       " '▁limitation',\n",
       " '▁may',\n",
       " '▁be',\n",
       " '▁express',\n",
       " 'ed',\n",
       " '▁only',\n",
       " '▁in',\n",
       " '▁terms',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁different',\n",
       " '▁levels',\n",
       " '▁of',\n",
       " '▁reaction',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'fire',\n",
       " '▁performance',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁products',\n",
       " '▁in',\n",
       " '▁their',\n",
       " '▁end',\n",
       " '-',\n",
       " 'use',\n",
       " '▁application',\n",
       " ';',\n",
       " '▁(5)',\n",
       " '▁By',\n",
       " '▁way',\n",
       " '▁of',\n",
       " '▁harmoni',\n",
       " 'sed',\n",
       " '▁solution',\n",
       " ',',\n",
       " '▁a',\n",
       " '▁system',\n",
       " '▁of',\n",
       " '▁classes',\n",
       " '▁was',\n",
       " '▁adopt',\n",
       " 'ed',\n",
       " '▁in',\n",
       " '▁Commission',\n",
       " '▁Deci',\n",
       " 'sion',\n",
       " '▁2000',\n",
       " '/',\n",
       " '147',\n",
       " '/',\n",
       " 'EC',\n",
       " '▁of',\n",
       " '▁8',\n",
       " '▁February',\n",
       " '▁2000',\n",
       " '▁implement',\n",
       " 'ing',\n",
       " '▁Council',\n",
       " '▁Direct',\n",
       " 'ive',\n",
       " '▁89',\n",
       " '/',\n",
       " '106',\n",
       " '/',\n",
       " 'EE',\n",
       " 'C',\n",
       " '▁as',\n",
       " '▁regard',\n",
       " 's',\n",
       " '▁the',\n",
       " '▁classifica',\n",
       " 'tion',\n",
       " '▁of',\n",
       " '▁the',\n",
       " '▁reaction',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'fire',\n",
       " '▁performance',\n",
       " '▁of',\n",
       " '▁construction',\n",
       " '▁products',\n",
       " '▁(3)',\n",
       " '.',\n",
       " '▁(6)',\n",
       " '▁In',\n",
       " '▁the',\n",
       " '▁case',\n",
       " '▁of',\n",
       " '▁wood',\n",
       " '▁floor',\n",
       " 'ing',\n",
       " '▁and',\n",
       " '▁solid',\n",
       " '▁wood',\n",
       " '▁panel',\n",
       " 'ling',\n",
       " '▁and',\n",
       " '▁cla',\n",
       " 'dding',\n",
       " '▁it',\n",
       " '▁is',\n",
       " '▁necessary',\n",
       " '▁to',\n",
       " '▁use',\n",
       " '▁the',\n",
       " '▁classifica',\n",
       " 'tion',\n",
       " '▁established',\n",
       " '▁in',\n",
       " '▁Deci',\n",
       " 'sion',\n",
       " '▁2000',\n",
       " '/',\n",
       " '147',\n",
       " '/',\n",
       " 'EC',\n",
       " '.',\n",
       " '▁(7)',\n",
       " '▁The',\n",
       " '▁reaction',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'fire',\n",
       " '▁performance',\n",
       " '▁of',\n",
       " '▁many',\n",
       " '▁construction',\n",
       " '▁products',\n",
       " '▁and',\n",
       " '/',\n",
       " 'or',\n",
       " '▁materials',\n",
       " ',',\n",
       " '▁within',\n",
       " '▁the',\n",
       " '▁classifica',\n",
       " 'tion',\n",
       " '▁provided',\n",
       " '▁for',\n",
       " '▁in',\n",
       " '▁Deci',\n",
       " 'sion',\n",
       " '▁2000',\n",
       " '/',\n",
       " '147',\n",
       " '/',\n",
       " 'EC',\n",
       " ',',\n",
       " '▁is',\n",
       " '▁well',\n",
       " '▁established',\n",
       " '▁and',\n",
       " '▁',\n",
       " 'sufficient',\n",
       " 'ly',\n",
       " '▁well',\n",
       " '▁known',\n",
       " '▁to',\n",
       " '▁fire',\n",
       " '▁regulator',\n",
       " 's',\n",
       " '▁in',\n",
       " '▁Member',\n",
       " '▁States',\n",
       " '▁that',\n",
       " '▁they',\n",
       " '▁do',\n",
       " '▁not',\n",
       " '▁require',\n",
       " '</s>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(train_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization check\n",
    "# Inspect output\n",
    "print(\"Original Text:\", train_dataset['text'][0])\n",
    "print(\"Token IDs:\", train_dataset[\"input_ids\"][0])\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(train_dataset[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9605ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 07:19:07.197053: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Convert datasets to TensorFlow Dataset\n",
    "def dataset_to_tf(dataset):\n",
    "    def gen():\n",
    "        for example in dataset:\n",
    "            yield {\n",
    "                \"input_ids\": example['input_ids'],\n",
    "                \"attention_mask\": example['attention_mask']\n",
    "            }, example['labels']\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec(shape=(512,), dtype=tf.int64),\n",
    "                \"attention_mask\": tf.TensorSpec(shape=(512,), dtype=tf.int64)\n",
    "            },\n",
    "            tf.TensorSpec(shape=(len(mlb.classes_),), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Convert both train and test datasets to TensorFlow Dataset\n",
    "train_tf_dataset = dataset_to_tf(train_dataset)\n",
    "test_tf_datasets = {lang: dataset_to_tf(test_datasets[lang]) for lang in test_datasets}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17fde1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class MicroF1(Metric):\n",
    "    def __init__(self, name='micro_f1', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = K.round(K.sigmoid(y_pred))  # Sigmoid activation for binary classification\n",
    "        y_true = K.cast(y_true, 'float32')   # Ensure the ground truth is float32\n",
    "\n",
    "        # Compute the True Positives, False Positives, and False Negatives\n",
    "        tp = K.sum(y_true * y_pred)\n",
    "        fp = K.sum((1 - y_true) * y_pred)\n",
    "        fn = K.sum(y_true * (1 - y_pred))\n",
    "\n",
    "        # Add to the running totals of tp, fp, and fn\n",
    "        self.tp.assign_add(tp)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.fn.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Precision and Recall calculations\n",
    "        precision = self.tp / (self.tp + self.fp + K.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + K.epsilon())\n",
    "\n",
    "        # F1 score computation\n",
    "        return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset the states to 0 at the start of each epoch\n",
    "        for var in [self.tp, self.fp, self.fn]:\n",
    "            var.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca8fb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.3.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.3.0 keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cb96a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9569adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b5926c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model for multi-label classification\n",
    "# Get the number of labels\n",
    "num_labels = len(mlb.classes_)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base', num_labels=num_labels, problem_type='multi_label_classification'\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model with appropriate loss and optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=tf.keras.metrics.AUC(multi_label=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_tf_dataset.batch(8), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f995c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model for each language\n",
    "for lang, tf_dataset in test_tf_datasets.items():\n",
    "    results = model.evaluate(tf_dataset.batch(8))\n",
    "    print(f\"Language: {lang}\")\n",
    "    print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "767b6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95073/2203363418.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m     18\u001b[39m trainer = Trainer(\n\u001b[32m     19\u001b[39m     model=model,\n\u001b[32m     20\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1368\u001b[39m, in \u001b[36mXLMRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1367\u001b[39m         loss_fct = BCEWithLogitsLoss()\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m         loss = \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1371\u001b[39m     output = (logits,) + outputs[\u001b[32m2\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:3643\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m   3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3644\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n\u001b[32m   3645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm-roberta-eurovoc\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_dir=\"./logs\",                    # Log directory\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_datasets[\"fr\"],  # Or \"de\", \"es\" — you can loop through them too\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
